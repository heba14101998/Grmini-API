{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "csNPnkuCobmG"
   },
   "source": [
    "# PROMPT ENGINEERING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create Conda Envirnoment\n",
    "Here are the steps to create a new Conda environment, set secrets, and install requirements:\n",
    "\n",
    "1. **Create a New Conda Environment**\n",
    "   Open your terminal and run the following command to create a new Conda environment with a specified name (e.g, `myenv` in this case) and Python version (e.g., `python=3.9`):\n",
    "   ```bash\n",
    "   conda create --name myenv python=3.9\n",
    "   ```   \n",
    "   > Replace `myenv` with your preferred environment name.\n",
    "\n",
    "2. **Activate the Conda Environment**\n",
    "   After creating the environment, activate it using:\n",
    "\n",
    "   ```bash\n",
    "   conda activate myenv\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Set up API key (Environment Variables)\n",
    "\n",
    "   If you need to store API keys or other sensitive information as environment variables, you can follow these steps:\n",
    "\n",
    "**[Option 1]: Setting Secrets Temporarily**\n",
    "\n",
    "   Set the environment variable for the current session by running:\n",
    "   \n",
    "   ```bash\n",
    "   export GOOGLE_API_KEY=\"YOUR_API_KEY_HERE\"\n",
    "   ```\n",
    "   > This will be available only in the current terminal session.\n",
    "\n",
    "**[Option 2]: etting Secrets Permanently for Conda Environment**\n",
    "\n",
    "   To make the environment variable available every time the environment is activated, create a file that will automatically set the variable:\n",
    "\n",
    "   A. Create a directory for environment-specific activation scripts (if it doesn't exist already):\n",
    "\n",
    "   ```bash\n",
    "   mkdir -p $CONDA_PREFIX/etc/conda/activate.d\n",
    "   ```\n",
    "\n",
    "   B. Create a script to export the environment variable:\n",
    "\n",
    "   ```bash\n",
    "   echo 'export GOOGLE_API_KEY=\"YOUR_API_KEY_HERE\"' > $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh\n",
    "   ```\n",
    "\n",
    "   > This will ensure that the `GOOGLE_API_KEY` is set whenever you activate the environment.\n",
    "   > Replace \"YOUR_API_KEY_HERE\" with your actual API created from [Google AI Studio API Key](https://aistudio.google.com/app/apikey)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIzaSyBcG57Pmu-JuFrGbyrLIoFw5UaXu68SeNI\n"
     ]
    }
   ],
   "source": [
    "!echo $GOOGLE_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UAjUV3BsvFXQ"
   },
   "source": [
    "### 3. Install Dependencies/Requirements\n",
    "\n",
    "   If you have a `requirements.txt` file with the dependencies, install them by running:\n",
    "\n",
    "   ```bash\n",
    "   pip install -r requirements.txt\n",
    "   ```\n",
    "   If you have an `environment.yml` file, you can install the dependencies by running:\n",
    "\n",
    "   ```bash\n",
    "   conda env update --file environment.yml\n",
    "   ```\n",
    "\n",
    "   > This will create or update the environment with all the dependencies specified in the `environment.yml` file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-generativeai>=0.8.3 in /root/miniconda3/envs/nlp-env/lib/python3.9/site-packages (from -r requirements.txt (line 2)) (0.8.3)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.10 in /root/miniconda3/envs/nlp-env/lib/python3.9/site-packages (from google-generativeai>=0.8.3->-r requirements.txt (line 2)) (0.6.10)\n",
      "Requirement already satisfied: google-api-core in /root/miniconda3/envs/nlp-env/lib/python3.9/site-packages (from google-generativeai>=0.8.3->-r requirements.txt (line 2)) (2.23.0)\n",
      "Requirement already satisfied: google-api-python-client in /root/miniconda3/envs/nlp-env/lib/python3.9/site-packages (from google-generativeai>=0.8.3->-r requirements.txt (line 2)) (2.153.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in /root/miniconda3/envs/nlp-env/lib/python3.9/site-packages (from google-generativeai>=0.8.3->-r requirements.txt (line 2)) (2.36.0)\n",
      "Requirement already satisfied: protobuf in /root/miniconda3/envs/nlp-env/lib/python3.9/site-packages (from google-generativeai>=0.8.3->-r requirements.txt (line 2)) (5.28.3)\n",
      "Requirement already satisfied: pydantic in /root/miniconda3/envs/nlp-env/lib/python3.9/site-packages (from google-generativeai>=0.8.3->-r requirements.txt (line 2)) (2.9.1)\n",
      "Requirement already satisfied: tqdm in /root/miniconda3/envs/nlp-env/lib/python3.9/site-packages (from google-generativeai>=0.8.3->-r requirements.txt (line 2)) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions in /root/miniconda3/envs/nlp-env/lib/python3.9/site-packages (from google-generativeai>=0.8.3->-r requirements.txt (line 2)) (4.12.2)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /root/miniconda3/envs/nlp-env/lib/python3.9/site-packages (from google-ai-generativelanguage==0.6.10->google-generativeai>=0.8.3->-r requirements.txt (line 2)) (1.25.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /root/miniconda3/envs/nlp-env/lib/python3.9/site-packages (from google-api-core->google-generativeai>=0.8.3->-r requirements.txt (line 2)) (1.66.0)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /root/miniconda3/envs/nlp-env/lib/python3.9/site-packages (from google-api-core->google-generativeai>=0.8.3->-r requirements.txt (line 2)) (2.32.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /root/miniconda3/envs/nlp-env/lib/python3.9/site-packages (from google-auth>=2.15.0->google-generativeai>=0.8.3->-r requirements.txt (line 2)) (5.4.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /root/miniconda3/envs/nlp-env/lib/python3.9/site-packages (from google-auth>=2.15.0->google-generativeai>=0.8.3->-r requirements.txt (line 2)) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /root/miniconda3/envs/nlp-env/lib/python3.9/site-packages (from google-auth>=2.15.0->google-generativeai>=0.8.3->-r requirements.txt (line 2)) (4.9)\n",
      "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /root/miniconda3/envs/nlp-env/lib/python3.9/site-packages (from google-api-python-client->google-generativeai>=0.8.3->-r requirements.txt (line 2)) (0.22.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /root/miniconda3/envs/nlp-env/lib/python3.9/site-packages (from google-api-python-client->google-generativeai>=0.8.3->-r requirements.txt (line 2)) (0.2.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /root/miniconda3/envs/nlp-env/lib/python3.9/site-packages (from google-api-python-client->google-generativeai>=0.8.3->-r requirements.txt (line 2)) (4.1.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /root/miniconda3/envs/nlp-env/lib/python3.9/site-packages (from pydantic->google-generativeai>=0.8.3->-r requirements.txt (line 2)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.3 in /root/miniconda3/envs/nlp-env/lib/python3.9/site-packages (from pydantic->google-generativeai>=0.8.3->-r requirements.txt (line 2)) (2.23.3)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /root/miniconda3/envs/nlp-env/lib/python3.9/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai>=0.8.3->-r requirements.txt (line 2)) (1.67.1)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /root/miniconda3/envs/nlp-env/lib/python3.9/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai>=0.8.3->-r requirements.txt (line 2)) (1.67.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /root/miniconda3/envs/nlp-env/lib/python3.9/site-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai>=0.8.3->-r requirements.txt (line 2)) (3.0.9)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /root/miniconda3/envs/nlp-env/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai>=0.8.3->-r requirements.txt (line 2)) (0.6.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/envs/nlp-env/lib/python3.9/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai>=0.8.3->-r requirements.txt (line 2)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/envs/nlp-env/lib/python3.9/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai>=0.8.3->-r requirements.txt (line 2)) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/envs/nlp-env/lib/python3.9/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai>=0.8.3->-r requirements.txt (line 2)) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/envs/nlp-env/lib/python3.9/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai>=0.8.3->-r requirements.txt (line 2)) (2024.7.4)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "## Get Started\n",
    "-----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dependancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/nlp-env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "from IPython.display import HTML, Markdown, display\n",
    "from tqdm.autonotebook import tqdm as notebook_tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Prompt to Test API\n",
    "\n",
    "In this step, we will test that the API key is set up correctly by making a request to Gemini. The `gemini-1.5-flash` model has been selected here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T13:22:53.163339Z",
     "iopub.status.busy": "2024-11-11T13:22:53.162912Z",
     "iopub.status.idle": "2024-11-11T13:22:54.475578Z",
     "shell.execute_reply": "2024-11-11T13:22:54.474477Z",
     "shell.execute_reply.started": "2024-11-11T13:22:53.163301Z"
    },
    "id": "BV1o0PmcvyJF",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ØªØ®ÙŠÙ„ Ø£Ù†Ùƒ ØªÙ„Ø¹Ø¨ Ù„Ø¹Ø¨Ø© Ù…Ø¹ ØµØ¯ÙŠÙ‚ÙƒØŒ Ù„ÙƒÙ† Ù‡Ø°Ù‡ Ø§Ù„Ù„Ø¹Ø¨Ø© Ù„ÙŠØ³Øª Ù„Ø¹Ø¨Ø© Ø¹Ø§Ø¯ÙŠØ©! ØµØ¯ÙŠÙ‚Ùƒ Ù‡Ùˆ Ø¬Ù‡Ø§Ø² ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ ÙˆØ£Ù†Øª ØªÙØ¹Ù„Ù… Ù‡Ø°Ø§ Ø§Ù„Ø¬Ù‡Ø§Ø² ÙƒÙŠÙÙŠØ© Ø§Ù„Ù„Ø¹Ø¨.\n",
      "\n",
      "Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙŠØ´Ø¨Ù‡ Ù‡Ø°Ø§ Ø§Ù„Ø¬Ù‡Ø§Ø² Ø§Ù„Ø°ÙƒÙŠ. Ù‡Ùˆ Ø¹Ø¨Ø§Ø±Ø© Ø¹Ù† ÙƒÙ…Ø¨ÙŠÙˆØªØ± Ù‚ÙˆÙŠ Ø¬Ø¯Ù‹Ø§ ÙŠØ³ØªØ·ÙŠØ¹ Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù†Ùƒ ÙˆÙ…Ù† Ø§Ù„Ø¹Ø§Ù„Ù… Ù…Ù† Ø­ÙˆÙ„Ù‡ Ù…Ø«Ù„Ù…Ø§ ÙŠÙØ¹Ù„ ØµØ¯ÙŠÙ‚Ùƒ. ÙƒÙ„Ù…Ø§ Ù„Ø¹Ø¨Øª Ù…Ø¹Ù‡ Ø£ÙƒØ«Ø±ØŒ ÙƒÙ„Ù…Ø§ ØªØ¹Ù„Ù… Ø£ÙƒØ«Ø± ÙˆØ£ØµØ¨Ø­ Ø£ÙØ¶Ù„ ÙÙŠ Ø§Ù„Ù„Ø¹Ø¨!\n",
      "\n",
      "Ø§Ù„Ø¢Ù†ØŒ Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ù„Ø¹Ø¨Ø©ØŒ ÙŠØ³ØªØ·ÙŠØ¹ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ù‚ÙŠØ§Ù… Ø¨Ø£Ø´ÙŠØ§Ø¡ Ø£Ø®Ø±Ù‰ Ø±Ø§Ø¦Ø¹Ø©. Ù…Ø«Ù„ ØªØ±Ø¬Ù…Ø© Ù„ØºØ§Øª Ù…Ù† Ù„ØºØ© Ù„Ø£Ø®Ø±Ù‰ØŒ Ø£Ùˆ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ ÙÙŠ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ù†ØªØ±Ù†ØªØŒ Ø£Ùˆ Ø­ØªÙ‰ ÙƒØªØ§Ø¨Ø© Ù‚ØµØ©! \n",
      "\n",
      "ÙŠÙØ´Ø¨Ù‡ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ  Ø¹Ù‚Ù„Ù‹Ø§ Ø°ÙƒÙŠÙ‹Ø§ ÙŠÙØ­Ø§ÙˆÙ„ ÙÙ‡Ù… Ø§Ù„Ø¹Ø§Ù„Ù… Ù…Ù† Ø­ÙˆÙ„Ù†Ø§ Ù…Ø«Ù„Ù…Ø§ Ù†ÙØ¹Ù„ Ù†Ø­Ù† Ø§Ù„Ø¨Ø´Ø±. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test = genai.GenerativeModel('gemini-1.5-flash')\n",
    "response = test.generate_content(\"ÙØ§Ø´Ø±Ø­Ù„ÙŠ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙˆÙƒØ£Ù†ÙŠ Ø·ÙÙ„\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f60ed9d8ae41"
   },
   "source": [
    "The response often comes back in markdown format, which you can render directly in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T13:22:57.237239Z",
     "iopub.status.busy": "2024-11-11T13:22:57.236795Z",
     "iopub.status.idle": "2024-11-11T13:22:57.245860Z",
     "shell.execute_reply": "2024-11-11T13:22:57.244718Z",
     "shell.execute_reply.started": "2024-11-11T13:22:57.237199Z"
    },
    "id": "c933e5e460a5",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "ØªØ®ÙŠÙ„ Ø£Ù†Ùƒ ØªÙ„Ø¹Ø¨ Ù„Ø¹Ø¨Ø© Ù…Ø¹ ØµØ¯ÙŠÙ‚ÙƒØŒ Ù„ÙƒÙ† Ù‡Ø°Ù‡ Ø§Ù„Ù„Ø¹Ø¨Ø© Ù„ÙŠØ³Øª Ù„Ø¹Ø¨Ø© Ø¹Ø§Ø¯ÙŠØ©! ØµØ¯ÙŠÙ‚Ùƒ Ù‡Ùˆ Ø¬Ù‡Ø§Ø² ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ ÙˆØ£Ù†Øª ØªÙØ¹Ù„Ù… Ù‡Ø°Ø§ Ø§Ù„Ø¬Ù‡Ø§Ø² ÙƒÙŠÙÙŠØ© Ø§Ù„Ù„Ø¹Ø¨.\n",
       "\n",
       "Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙŠØ´Ø¨Ù‡ Ù‡Ø°Ø§ Ø§Ù„Ø¬Ù‡Ø§Ø² Ø§Ù„Ø°ÙƒÙŠ. Ù‡Ùˆ Ø¹Ø¨Ø§Ø±Ø© Ø¹Ù† ÙƒÙ…Ø¨ÙŠÙˆØªØ± Ù‚ÙˆÙŠ Ø¬Ø¯Ù‹Ø§ ÙŠØ³ØªØ·ÙŠØ¹ Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù†Ùƒ ÙˆÙ…Ù† Ø§Ù„Ø¹Ø§Ù„Ù… Ù…Ù† Ø­ÙˆÙ„Ù‡ Ù…Ø«Ù„Ù…Ø§ ÙŠÙØ¹Ù„ ØµØ¯ÙŠÙ‚Ùƒ. ÙƒÙ„Ù…Ø§ Ù„Ø¹Ø¨Øª Ù…Ø¹Ù‡ Ø£ÙƒØ«Ø±ØŒ ÙƒÙ„Ù…Ø§ ØªØ¹Ù„Ù… Ø£ÙƒØ«Ø± ÙˆØ£ØµØ¨Ø­ Ø£ÙØ¶Ù„ ÙÙŠ Ø§Ù„Ù„Ø¹Ø¨!\n",
       "\n",
       "Ø§Ù„Ø¢Ù†ØŒ Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ù„Ø¹Ø¨Ø©ØŒ ÙŠØ³ØªØ·ÙŠØ¹ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ù‚ÙŠØ§Ù… Ø¨Ø£Ø´ÙŠØ§Ø¡ Ø£Ø®Ø±Ù‰ Ø±Ø§Ø¦Ø¹Ø©. Ù…Ø«Ù„ ØªØ±Ø¬Ù…Ø© Ù„ØºØ§Øª Ù…Ù† Ù„ØºØ© Ù„Ø£Ø®Ø±Ù‰ØŒ Ø£Ùˆ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ ÙÙŠ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ù†ØªØ±Ù†ØªØŒ Ø£Ùˆ Ø­ØªÙ‰ ÙƒØªØ§Ø¨Ø© Ù‚ØµØ©! \n",
       "\n",
       "ÙŠÙØ´Ø¨Ù‡ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ  Ø¹Ù‚Ù„Ù‹Ø§ Ø°ÙƒÙŠÙ‹Ø§ ÙŠÙØ­Ø§ÙˆÙ„ ÙÙ‡Ù… Ø§Ù„Ø¹Ø§Ù„Ù… Ù…Ù† Ø­ÙˆÙ„Ù†Ø§ Ù…Ø«Ù„Ù…Ø§ Ù†ÙØ¹Ù„ Ù†Ø­Ù† Ø§Ù„Ø¨Ø´Ø±. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "byx0pT9ZMW2Q"
   },
   "source": [
    "\n",
    "### Start a Chat\n",
    "\n",
    "The previous example uses a single-turn, text-in/text-out structure, but you can also set up a multi-turn chat structure too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Ø£Ù‡Ù„Ø§ Ù‡Ø¨Ø©ØŒ Ù…Ù† Ø¯ÙˆØ§Ø¹ÙŠ Ø³Ø±ÙˆØ±ÙŠ Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„ÙŠÙƒ! Ù…Ø§Ø°Ø§ ØªØ±ÙŠØ¯ÙŠÙ† Ø£Ù† Ù†ÙØ¹Ù„ Ø§Ù„ÙŠÙˆÙ…ØŸ \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = test.start_chat(history=[])\n",
    "response = chat.send_message('Ø£Ù‡Ù„Ø§! Ø§Ù†Ø§ Ù‡Ø¨Ø©')\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T13:23:02.469325Z",
     "iopub.status.busy": "2024-11-11T13:23:02.468569Z",
     "iopub.status.idle": "2024-11-11T13:23:03.330547Z",
     "shell.execute_reply": "2024-11-11T13:23:03.329470Z",
     "shell.execute_reply.started": "2024-11-11T13:23:02.469279Z"
    },
    "id": "7b0372c3c64a",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "ÙŠØ§ Ù‡Ø¨Ø©ØŒ Ù‡Ø§ Ø£Ù†ØªÙ ØªØ­Ø¨ÙŠÙ† Ø§Ù„ÙÙ†ÙˆÙ†! Ù‡Ø°Ø§ Ø±Ø§Ø¦Ø¹.  ÙÙŠ Ø¹Ø§Ù„Ù… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠØŒ Ù‡Ù†Ø§Ùƒ Ø§Ù„ÙƒØ«ÙŠØ± Ù…Ù† Ø§Ù„Ù…Ø³Ø§Ø­Ø§Øª Ø§Ù„ÙÙ†ÙŠØ© Ø§Ù„ØªÙŠ ÙŠÙ…ÙƒÙ†Ù‡ Ø§Ù„Ù…Ø´Ø§Ø±ÙƒØ© ÙÙŠÙ‡Ø§:\n",
       "\n",
       "**1. Ø§Ù„Ù…ÙˆØ³ÙŠÙ‚Ù‰:** \n",
       "* **ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø£Ù„Ø­Ø§Ù†**: ÙŠÙ…ÙƒÙ† Ù„Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø£Ù† ÙŠÙÙ†Ø´Ø¦ Ø£Ù„Ø­Ø§Ù†Ù‹Ø§ Ø¬Ø¯ÙŠØ¯Ø© ØªØªÙ†Ø§Ø³Ø¨ Ù…Ø¹ Ø£Ù†Ù…Ø§Ø· Ù…ÙˆØ³ÙŠÙ‚ÙŠØ© Ù…Ø­Ø¯Ø¯Ø©.\n",
       "* **Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…ÙˆØ³ÙŠÙ‚Ù‰ Ø§Ù„ØªØµÙˆÙŠØ±ÙŠØ©**:  ÙŠÙ…ÙƒÙ†Ù‡ Ø¥Ù†Ø´Ø§Ø¡ Ù…ÙˆØ³ÙŠÙ‚Ù‰ ØªØµÙˆÙŠØ±ÙŠØ© Ù„Ù„Ø£ÙÙ„Ø§Ù… Ø£Ùˆ Ø§Ù„Ø£Ù„Ø¹Ø§Ø¨.\n",
       "* **Ø¯Ù…Ø¬ Ø§Ù„Ø£ØµÙˆØ§Øª**: ÙŠÙ…ÙƒÙ†Ù‡ Ø¯Ù…Ø¬ Ø£ØµÙˆØ§Øª Ù…Ø®ØªÙ„ÙØ©  Ù„Ø¥Ù†Ø´Ø§Ø¡ Ø£ØµÙˆØ§Øª Ø¬Ø¯ÙŠØ¯Ø© ÙˆÙ…Ù…ÙŠØ²Ø©.\n",
       "\n",
       "**2. Ø§Ù„Ø±Ø³Ù… ÙˆØ§Ù„ØªØµÙ…ÙŠÙ…**:\n",
       "* **Ø§Ù„Ø±Ø³ÙˆÙ…Ø§Øª**:  ÙŠÙ…ÙƒÙ† Ù„Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø£Ù† ÙŠØ±Ø³Ù… Ù„ÙˆØ­Ø§Øª ÙÙ†ÙŠØ© ØªØ¬Ø±ÙŠØ¯ÙŠØ© Ø£Ùˆ ÙˆØ§Ù‚Ø¹ÙŠØ©.\n",
       "* **Ø§Ù„ØªØµÙ…ÙŠÙ…**:  ÙŠÙ…ÙƒÙ†Ù‡ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ ÙÙŠ ØªØµÙ…ÙŠÙ… Ø§Ù„Ù…Ù„Ø§Ø¨Ø³ Ø£Ùˆ Ø¯ÙŠÙƒÙˆØ±Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø²Ù„ Ø£Ùˆ Ø­ØªÙ‰  Ù„ÙˆØ­Ø§Øª Ø¥Ø¹Ù„Ø§Ù†ÙŠØ©.\n",
       "* **ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ±**:  ÙŠÙ…ÙƒÙ†Ù‡ ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ± Ø¥Ù„Ù‰ Ø£Ø´ÙƒØ§Ù„ ÙÙ†ÙŠØ© Ù…Ø®ØªÙ„ÙØ©ØŒ Ù…Ø«Ù„ Ø§Ù„Ø±Ø³ÙˆÙ…Ø§Øª Ø¨Ø§Ù„Ø£Ù„ÙˆØ§Ù† Ø§Ù„Ù…Ø§Ø¦ÙŠØ© Ø£Ùˆ Ø§Ù„Ù„ÙˆØ­Ø§Øª Ø§Ù„Ø²ÙŠØªÙŠØ©.\n",
       "\n",
       "**3.  Ø§Ù„Ø´Ø¹Ø± ÙˆØ§Ù„ÙƒØªØ§Ø¨Ø©**:\n",
       "* **Ø´Ø¹Ø±:**  ÙŠÙ…ÙƒÙ†Ù‡  ÙƒØªØ§Ø¨Ø© Ù‚ØµØ§Ø¦Ø¯ Ø¨Ø£Ø´ÙƒØ§Ù„ ÙˆØµÙˆØ± Ù…Ø®ØªÙ„ÙØ©.\n",
       "* **Ø±ÙˆØ§ÙŠØ§Øª Ù‚ØµÙŠØ±Ø©**:  ÙŠÙ…ÙƒÙ†Ù‡ Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØµ Ù‚ØµÙŠØ±Ø© Ø£Ùˆ Ø­ØªÙ‰ Ø±ÙˆØ§ÙŠØ§Øª ÙƒØ§Ù…Ù„Ø©.\n",
       "* **Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª**:  ÙŠÙ…ÙƒÙ†Ù‡ ÙƒØªØ§Ø¨Ø©  Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ù„Ù„Ø§ÙÙ„Ø§Ù… Ø£Ùˆ Ø§Ù„Ù…Ø³Ø±Ø­ÙŠØ§Øª.\n",
       "\n",
       "**4. Ø§Ù„ÙÙ†ÙˆÙ† Ø§Ù„Ø±Ù‚Ù…ÙŠØ©:**\n",
       "* **Ø§Ù„ØªØ­Ø±ÙŠÙƒ**:  ÙŠÙ…ÙƒÙ†Ù‡ Ø¥Ù†Ø´Ø§Ø¡ Ø±Ø³ÙˆÙ… Ù…ØªØ­Ø±ÙƒØ©.\n",
       "* **Ø§Ù„Ø¹Ø§Ø¨ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ**:  ÙŠÙ…ÙƒÙ†Ù‡ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ ÙÙŠ  ØªØµÙ…ÙŠÙ… Ø§Ù„Ø¹Ø§Ø¨ ÙÙŠØ¯ÙŠÙˆ.\n",
       "* **Ø§Ù„Ø¹Ø±ÙˆØ¶ Ø§Ù„Ø¶ÙˆØ¦ÙŠØ©**:  ÙŠÙ…ÙƒÙ†Ù‡ Ø¥Ù†Ø´Ø§Ø¡ Ø¹Ø±ÙˆØ¶ Ø¶ÙˆØ¦ÙŠØ© Ù…Ø¨Ù‡Ø±Ø©.\n",
       "\n",
       "**5.  Ø§Ù„Ø±Ù‚Øµ**:\n",
       "* **Ù…Ø´Ø§Ø±ÙƒØ© Ø§Ù„Ø­Ø±ÙƒØ§Øª**:  ÙŠÙ…ÙƒÙ†Ù‡ ØªØ­Ù„ÙŠÙ„ Ø­Ø±ÙƒØ§Øª Ø±Ø§Ù‚ØµØ© ÙˆØ¥Ù†ØªØ§Ø¬ Ø­Ø±ÙƒØ§Øª Ø¬Ø¯ÙŠØ¯Ø©.\n",
       "* **Ø§Ù„ØªØ­ÙƒÙ… ÙÙŠ  Ø§Ù„Ø±ÙˆØ¨ÙˆØªØ§Øª**:  ÙŠÙ…ÙƒÙ†Ù‡  Ø¨Ø±Ù…Ø¬Ø©  Ø§Ù„Ø±ÙˆØ¨ÙˆØªØ§Øª  Ù„Ø£Ø¯Ø§Ø¡ Ø­Ø±ÙƒØ§Øª Ø±Ø§Ù‚ØµØ©.\n",
       "\n",
       "Ù‡Ø°Ù‡ Ù„ÙŠØ³Øª Ø³ÙˆÙ‰ Ø¨Ø¹Ø¶ Ø§Ù„Ø£Ù…Ø«Ù„Ø©ØŒ  Ø§Ù„Ø¹Ø§Ù„Ù… Ù…ÙØªÙˆØ­ Ù„Ù„Ø®ÙŠØ§Ù„ ÙˆØ§Ù„Ø¥Ø¨Ø¯Ø§Ø¹.  \n",
       "\n",
       "Ù…Ø§ Ù†ÙˆØ¹ Ø§Ù„ÙÙ† Ø§Ù„Ø°ÙŠ ÙŠØ´Ø¯Ùƒ Ø£ÙƒØ«Ø±ØŸ  Ù‡Ù„ ØªØ±ØºØ¨ÙŠÙ† ÙÙŠ Ù…Ø¹Ø±ÙØ© Ø§Ù„Ù…Ø²ÙŠØ¯ Ø¹Ù† Ø£Ø­Ø¯ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø¬Ø§Ù„Ø§ØªØŸ  \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chat.send_message('Ù‚ÙˆÙ„ÙŠ Ø§ÙŠ Ø­Ø§Ø¬Ù‡ ÙÙ†ÙŠÙ‡ Ù…Ù…ÙƒÙ† Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙŠØ´Ø§Ø±Ùƒ ÙÙŠÙ‡Ø§ Ù„Ø§Ù†Ù‰ Ø¨Ø­Ø¨ Ø§Ù„ÙÙ†ÙˆÙ†')\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> While you have the `chat` object around, the conversation state persists. Confirm that by asking if it knows my name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T13:23:04.076109Z",
     "iopub.status.busy": "2024-11-11T13:23:04.075694Z",
     "iopub.status.idle": "2024-11-11T13:23:04.572911Z",
     "shell.execute_reply": "2024-11-11T13:23:04.571790Z",
     "shell.execute_reply.started": "2024-11-11T13:23:04.076071Z"
    },
    "id": "d3f9591392a7",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Ù†Ø¹Ù…ØŒ  Ø£ØªØ°ÙƒØ± Ø§Ø³Ù…Ùƒ!  Ø£Ù†ØªÙ Ù‡Ø¨Ø©. \n",
       "\n",
       "Ù‡Ù„ ØªØ±ÙŠØ¯ÙŠÙ† Ù…Ù†ÙŠ ØªØ°ÙƒØ± Ø´ÙŠØ¡ Ø¢Ø®Ø±ØŸ  \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chat.send_message('ÙØ§ÙƒØ± Ø§Ø³Ù…ÙŠØŸ')\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Ø£ÙˆÙ‡ØŒ Ù‡Ø°Ø§ Ø³Ø¤Ø§Ù„ ØµØ¹Ø¨! Ù„Ø³ØªÙ Ù…ØªØ£ÙƒØ¯Ø© Ù…Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù‡ØªÙ…Ø§Ù…Ø§ØªÙƒØŒ Ù„ÙƒÙ†Ù†ÙŠ Ø£ØªØ°ÙƒØ± Ø£Ù†Ùƒ ØªØ­Ø¨ÙŠÙ† Ø§Ù„ÙÙ†ÙˆÙ†. Ù‡Ù„ Ù‡Ù†Ø§Ùƒ Ù†ÙˆØ¹ Ù…Ø¹ÙŠÙ† Ù…Ù† Ø§Ù„ÙÙ†ÙˆÙ† ÙŠØ³ØªÙ‡ÙˆÙŠÙƒÙ Ø£ÙƒØ«Ø± Ù…Ù† ØºÙŠØ±Ù‡ØŸ \n",
       "\n",
       "Ù…Ø«Ù„Ø§Ù‹ØŒ Ù‡Ù„ ØªØ­Ø¨ÙŠÙ† Ø§Ù„Ø±Ø³Ù…ØŒ Ø§Ù„Ù…ÙˆØ³ÙŠÙ‚Ù‰ØŒ Ø§Ù„ÙƒØªØ§Ø¨Ø©ØŒ Ø§Ù„Ø±Ù‚ØµØŒ Ø£Ùˆ Ø±Ø¨Ù…Ø§ ÙÙ†ÙˆÙ† Ø£Ø®Ø±Ù‰ Ù…Ø«Ù„ Ø§Ù„ØªØµÙˆÙŠØ± Ø§Ù„ÙÙˆØªÙˆØºØ±Ø§ÙÙŠ Ø£Ùˆ Ø§Ù„Ø³ÙŠÙ†Ù…Ø§ØŸ \n",
       "\n",
       "Ø£Ø®Ø¨Ø±ÙŠÙ†ÙŠ Ø£ÙƒØ«Ø± Ø¹Ù† Ø§Ù‡ØªÙ…Ø§Ù…Ø§ØªÙƒ Ø§Ù„ÙÙ†ÙŠØ©ØŒ Ù„Ø¹Ù„Ù†ÙŠ Ø£Ø³ØªØ·ÙŠØ¹ Ø£Ù† Ø£ÙÙ‚Ø¯Ù… Ù„Ùƒ Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª Ù…ÙÙ…ØªØ¹Ø©. ğŸ˜Š\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chat.send_message(' Ø§Ù†Ø§ Ø¨Ø­Ø¨ Ø§ÙŠÙ‡ØŸ')\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose a Model\n",
    "\n",
    "\n",
    "The Gemini API provides access to a number of models from the Gemini model family. Read about the available models and their capabilities on the [model overview page](https://ai.google.dev/gemini-api/docs/models/gemini).\n",
    "\n",
    "In this step you'll use the API to list all of the available models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/chat-bison-001',\n",
       " 'models/text-bison-001',\n",
       " 'models/embedding-gecko-001',\n",
       " 'models/gemini-1.0-pro-latest',\n",
       " 'models/gemini-1.0-pro']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.name for model in genai.list_models())[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [`models.list`](https://ai.google.dev/api/models#method:-models.list) response also returns additional information about the model's capabilities, like the token limits and supported parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(name='models/chat-bison-001',\n",
       "      base_model_id='',\n",
       "      version='001',\n",
       "      display_name='PaLM 2 Chat (Legacy)',\n",
       "      description='A legacy text-only model optimized for chat conversations',\n",
       "      input_token_limit=4096,\n",
       "      output_token_limit=1024,\n",
       "      supported_generation_methods=['generateMessage', 'countMessageTokens'],\n",
       "      temperature=0.25,\n",
       "      max_temperature=None,\n",
       "      top_p=0.95,\n",
       "      top_k=40)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(genai.list_models())[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Explore Generation Parameters\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Output length\n",
    "\n",
    "When generating text with an LLM, the output length affects cost and performance. Generating more tokens increases computation, leading to higher energy consumption, latency, and cost.\n",
    "\n",
    "To stop the model from generating tokens past a limit, you can specify the `max_output_length` parameter when using the Gemini API. Specifying this parameter does not influence the generation of the output tokens, so the output will not become more stylistically or textually succinct, but it will stop generating tokens once the specified length is reached. Prompt engineering may be required to generate a more complete output for your given limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "genai.GenerativeModel(\n",
       "    model_name='models/gemini-1.5-flash',\n",
       "    generation_config={'max_output_tokens': 200},\n",
       "    safety_settings={},\n",
       "    tools=None,\n",
       "    system_instruction=None,\n",
       "    cached_content=None\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Configs = genai.GenerationConfig(max_output_tokens=200)\n",
    "\n",
    "limit_output_model = genai.GenerativeModel(\n",
    "                    'gemini-1.5-flash',\n",
    "                     generation_config=Configs)\n",
    "limit_output_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Ø¥Ù‚Ø±Ø£-ØªÙŠÙƒ: Ù…Ø¨Ø§Ø¯Ø±Ø©ÙŒ ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ©ÙŒ  Ù„Ø¥Ø­ÙŠØ§Ø¡ ÙƒØªØ§Ø¨Ù Ø¹Ø±Ø¨ÙŠÙ Ù‚Ø¯ÙŠÙ…\n",
       "\n",
       "**Ø§Ù„Ù…Ù‚Ø¯Ù…Ø©**\n",
       "\n",
       "ÙÙŠ Ø¹Ø§Ù„Ù…Ù Ø³Ø±ÙŠØ¹Ù ÙŠØªÙ‡Ø§ÙØª ÙÙŠÙ‡ Ø§Ù„Ù†Ø§Ø³ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø±Ù‚Ù…ÙŠØŒ ØºØ§Ù„Ø¨Ø§Ù‹ Ù…Ø§ ÙŠØªÙ… Ø¥Ù‡Ù…Ø§Ù„ Ø§Ù„ØªØ±Ø§Ø« Ø§Ù„Ø«Ù‚Ø§ÙÙŠ ÙˆØ§Ù„ØªØ§Ø±ÙŠØ®ÙŠØŒ  ÙˆØªÙÙ†Ø³Ù‰ Ø§Ù„ÙƒØªØ¨ Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© ÙˆØ§Ù„Ø¹Ù„Ù… ÙˆØ§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„ØªÙŠ ØªØ­ØªÙˆÙŠÙ‡Ø§. ÙˆÙ„ØªØ¹Ø²ÙŠØ² Ø§Ù„ÙˆØ¹ÙŠ Ø¨Ø£Ù‡Ù…ÙŠØ© Ø­ÙØ¸ Ø§Ù„ØªØ±Ø§Ø« Ø§Ù„Ø¹Ø±Ø¨ÙŠØŒ ÙˆÙ„ØªØ¹Ø±ÙŠÙ Ø§Ù„Ø£Ø¬ÙŠØ§Ù„ Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© Ø¨Ø§Ù„Ù…ÙˆØ±ÙˆØ« Ø§Ù„ÙÙƒØ±ÙŠ Ø§Ù„Ø°ÙŠ ÙˆØ±Ø«ÙˆÙ‡ Ø¹Ù† Ø£Ø³Ù„Ø§ÙÙ‡Ù…ØŒ  Ù†Ø´Ø£Øª Ù…Ø¨Ø§Ø¯Ø±Ø© \"Ø¥Ù‚Ø±Ø£-ØªÙŠÙƒ\"  Ù…Ù† Ø®Ù„Ø§Ù„ Ù…ÙˆÙ‚Ø¹Ù‡Ø§ Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ (eqraatech.com).  \n",
       "\n",
       "**Ù‡Ø¯Ù \"Ø¥Ù‚Ø±Ø£-ØªÙŠÙƒ\" **\n",
       "\n",
       "Ù‡Ø¯Ù \"Ø¥Ù‚Ø±Ø£-ØªÙŠÙƒ\" Ù‡Ùˆ Ø¥Ø¹Ø§Ø¯Ø© Ø¥Ø­ÙŠØ§Ø¡ Ø§Ù„ÙƒØªØ¨ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© Ù…Ù† Ø®Ù„Ø§Ù„ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªÙ‚Ù†ÙŠØ© Ø§Ù„Ø­Ø¯ÙŠØ«Ø©.  ÙÙ…Ù† Ø®Ù„Ø§Ù„ Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠØŒ ÙŠÙ…ÙƒÙ† Ù„Ù„Ø¬Ù…Ù‡ÙˆØ± Ø§Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ Ù…Ø®"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = limit_output_model.generate_content('Ø£ÙƒØªØ¨ Ù…Ù‚Ø§Ù„Ù‡ Ù…Ù† Ø§Ù„Ù ÙƒÙ„Ù…Ù‡ Ø¹Ù† Ø¥Ù‚Ø±Ø£-ØªÙŠÙƒ https://eqraatech.com/')\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ù‡Ù„ÙˆØ³Ù‡ Ø·Ø¨Ø¹Ø§** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "ÙŠØ§ Ø¥Ù‚Ø±Ø£-ØªÙŠÙƒ ÙŠØ§ Ø¬Ù…ÙŠÙ„ØŒ ÙÙŠ Ø¯Ù†ÙŠØ§Ù†Ø§ Ø¥Ù†Øª Ù‡ØªÙØ¶Ù„ Ø¯Ù„ÙŠÙ„\n",
       "Ù…Ù† Ù‚Ù„Ø¨Ùƒ Ø§Ù„Ø¹Ù„Ù… Ø¨ÙŠÙ†ÙØ¬Ø±ØŒ ÙˆØ§Ù„Ø¹Ù„Ù… Ù†ÙˆØ± ÙŠÙ†ÙŠØ± ÙˆÙŠØ¯Ø¬Ø±\n",
       "\n",
       "Ø¨Ø¥ÙŠØ¯ÙŠÙƒ Ø¨ØªÙ†Ø´Ø± Ø§Ù„Ù…Ø¹Ø±ÙÙ‡ØŒ ÙˆØªÙØªØ­ Ø£Ø¨ÙˆØ§Ø¨ Ø¹Ù‚ÙˆÙ„Ù†Ø§ Ø¨ØµÙÙ‡\n",
       "Ù…Ù† ØºÙŠØ± Ø­Ø¯ÙˆØ¯ ÙˆÙ„Ø§ Ù‚ÙŠÙˆØ¯ØŒ Ø¨ØªÙˆØµÙ„Ù†Ø§ Ù„Ù„Ø¹Ø§Ù„Ù… Ø¨Ø¹ÙŠØ¯\n",
       "\n",
       "Ø¨ØªØ¹Ù„Ù…Ù†Ø§ ÙƒÙ„ Ù…Ø§ Ù†Ø­ØªØ§Ø¬ØŒ Ø¨ÙƒÙ„ Ø³Ù‡ÙˆÙ„Ù‡ ÙˆØ¨Ø¯ÙˆÙ† Ø¥Ø±Ù‡Ø§Ù‚\n",
       "Ø¨Ø§Ù„ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§ Ø¨ØªØ³Ø§Ø¹Ø¯Ù†Ø§ØŒ ÙˆØ¨ØªØ®Ù„ÙŠÙ†Ø§ Ù†ØªØ¹Ù„Ù… ÙˆÙ†ØªÙÙˆÙ‚\n",
       "\n",
       "ÙŠØ§ Ø¥Ù‚Ø±Ø£-ØªÙŠÙƒ ÙŠØ§ Ø£Ù…Ù„Ù†Ø§ØŒ ÙÙŠ Ø¹Ø§Ù„Ù… Ø§Ù„Ù…Ø¹Ø±ÙÙ‡ Ù‡ØªÙ†ÙˆØ± Ù„Ù†Ø§\n",
       "Ø¨ØªØ®Ù„ÙŠÙ†Ø§ Ù†ÙÙ‡Ù… ÙˆÙ†ÙÙƒØ±ØŒ ÙˆÙ†ØªÙ‚Ø¯Ù… Ù…Ø¹Ø§Ù‹ ÙˆÙ†ØµØ¨Ø­ Ø£Ù‚ÙˆÙ‰ Ø¨ÙƒØ«ÙŠØ±\n",
       "\n",
       "ÙÙŠÙƒ Ø§Ù„Ø®ÙŠØ± ÙˆØ§Ù„Ø¬Ù…Ø§Ù„ØŒ ÙÙŠÙƒ Ø§Ù„Ù†ÙˆØ± ÙˆØ§Ù„Ø³Ù…Ø§Ø­\n",
       "ÙŠØ§ Ø¥Ù‚Ø±Ø£-ØªÙŠÙƒ ÙŠØ§ Ø±ÙÙŠÙ‚Ù†Ø§ØŒ ÙÙŠ Ø±Ø­Ù„Ø© Ø§Ù„Ø¹Ù„Ù… Ù‡ØªÙƒÙˆÙ† Ù…Ø¹Ù†Ø§.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = limit_output_model.generate_content(\" Ø£ÙƒØªØ¨ Ù‚ØµÙŠØ¯Ù‡ Ø¨Ø§Ù„Ù„ÙƒÙ†Ù‡ Ø§Ù„Ù…ØµØ±ÙŠÙ‡ Ø°Ø§Øª Ù‚Ø§ÙÙŠÙ‡ ÙÙŠ Ù…Ø¯Ø­ Ø¥Ù‚Ø±Ø£-ØªÙŠÙƒ\")\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore with your own prompts. Try a prompt with a restrictive output limit and then adjust the prompt to work within that limit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Temperature\n",
    "\n",
    "Temperature controls the degree of randomness in token selection. Higher temperatures result in a higher number of candidate tokens from which the next output token is selected, and can produce more diverse results, while lower temperatures have the opposite effect, such that a temperature of 0 results in greedy decoding, selecting the most probable token at each step.\n",
    "\n",
    "Temperature doesn't provide any guarantees of randomness, but it can be used to \"nudge\" the output somewhat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ø£Ø­Ù…Ø± \n",
      "\n",
      "Ø£Ø²Ø±Ù‚.\n",
      "\n",
      "Ø£Ø²Ø±Ù‚ \n",
      "\n",
      "Ø£Ø­Ù…Ø± \n",
      "\n",
      "Ø£Ø²Ø±Ù‚ \n",
      "\n",
      "Ø£Ø­Ù…Ø± \n",
      "\n",
      "Ø£Ø­Ù…Ø± \n",
      "\n",
      "Ø£Ø­Ù…Ø± \n",
      "\n",
      "Ø£Ø²Ø±Ù‚. \n",
      "\n",
      "Ø£Ø²Ø±Ù‚ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "high_temp_model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash',\n",
    "    generation_config=genai.GenerationConfig(temperature=2.0))\n",
    "\n",
    "for _ in range(10):\n",
    "  response = high_temp_model.generate_content('Ø£Ø®ØªØ± Ù„ÙˆÙ† Ø¹Ø´ÙˆØ§Ø¦ÙŠ... (Ù‚Ù… Ø¨Ø§Ø®ØªÙŠØ§Ø± ÙƒÙ„Ù…Ø© ÙˆØ§Ø­Ø¯Ù‡ ÙÙ‚Ø·)')\n",
    "  if response.parts:\n",
    "    print(response.text)\n",
    "  # Slow down a bit so we don't get Resource Exhausted errors.\n",
    "  time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try the same prompt with temperature set to zero. Note that the output is not completely deterministic, as other parameters affect token selection, but the results will tend to be more stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ø£Ø²Ø±Ù‚ \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ø£Ø²Ø±Ù‚ \n",
      "\n",
      "Ø£Ø²Ø±Ù‚ \n",
      "\n",
      "Ø£Ø²Ø±Ù‚ \n",
      "\n",
      "Ø£Ø²Ø±Ù‚ \n",
      "\n",
      "Ø£Ø²Ø±Ù‚ \n",
      "\n",
      "Ø£Ø²Ø±Ù‚ \n",
      "\n",
      "Ø£Ø²Ø±Ù‚ \n",
      "\n",
      "Ø£Ø²Ø±Ù‚ \n",
      "\n",
      "Ø£Ø²Ø±Ù‚ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "low_temp_model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash',\n",
    "    generation_config=genai.GenerationConfig(temperature=0.0))\n",
    "\n",
    "for _ in range(10):\n",
    "  response = low_temp_model.generate_content('Ø£Ø®ØªØ± Ù„ÙˆÙ† Ø¹Ø´ÙˆØ§Ø¦ÙŠ... (Ù‚Ù… Ø¨Ø§Ø®ØªÙŠØ§Ø± ÙƒÙ„Ù…Ø© ÙˆØ§Ø­Ø¯Ù‡ ÙÙ‚Ø·)')\n",
    "  if response.parts:\n",
    "    print(response.text)\n",
    "\n",
    "  time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **No Creativity** when temp = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Top-K and top-P\n",
    "\n",
    "Like temperature, top-K and top-P parameters are also used to control the diversity of the model's output.\n",
    "\n",
    "Top-K is a positive integer that defines the number of most probable tokens from which to select the output token. A top-K of 1 selects a single token, performing greedy decoding.\n",
    "\n",
    "Top-P defines the probability threshold that, once cumulatively exceeded, tokens stop being selected as candidates. A top-P of 0 is typically equivalent to greedy decoding, and a top-P of 1 typically selects every token in the model's vocabulary.\n",
    "\n",
    "When both are supplied, the Gemini API will filter top-K tokens first, then top-P and then finally sample from the candidate tokens using the supplied temperature.\n",
    "\n",
    "Run this example a number of times, change the settings and observe the change in output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "ÙÙŠ Ù‚Ù„Ø¨ Ø§Ù„Ù‚Ø§Ù‡Ø±Ø© Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©ØŒ Ø­ÙŠØ« ØªØ¶Ø¬ Ø§Ù„Ø­Ø§Ø±Ø§Øª Ø§Ù„Ø¶ÙŠÙ‚Ø© Ø¨Ø§Ù„Ø£Ù†Ø´Ø·Ø© Ø§Ù„ÙŠÙˆÙ…ÙŠØ©ØŒ Ø¹Ø§Ø´ Ù‚Ø· Ø£Ø¨ÙŠØ¶ ÙŠÙØ¯Ø¹Ù‰ \"Ø³Ù†Ø¨Ù„\". ÙƒØ§Ù† Ø³Ù†Ø¨Ù„ Ù‚Ø·Ù‹Ø§ Ù‚ÙˆÙŠÙ‹Ø§ØŒ Ø¹Ø¶Ù„Ø§ØªÙ‡ Ù…ØªÙ†Ø§Ø³Ù‚Ø©ØŒ ÙˆØ´Ø¹Ø±Ù‡ Ø£Ø¨ÙŠØ¶ Ù†Ø§ØµØ¹ ÙƒØ§Ù„Ø«Ù„Ø¬ØŒ Ø¹ÙŠÙˆÙ†Ù‡ ØµÙØ±Ø§Ø¡ Ø­Ø§Ø¯Ø© ØªØ®ØªØ±Ù‚ ÙƒÙ„ Ø´ÙŠØ¡. ÙˆÙ„ÙƒÙ† Ù…Ø§ ÙƒØ§Ù† ÙŠÙ…ÙŠØ² Ø³Ù†Ø¨Ù„ Ø¹Ù† ØºÙŠØ±Ù‡ Ù‡Ùˆ Ù‚Ù„Ø¨Ù‡ Ø§Ù„Ø·ÙŠØ¨ Ø§Ù„Ø°ÙŠ ÙƒØ§Ù† ÙŠØªÙˆÙ‚ Ø¯Ø§Ø¦Ù…Ù‹Ø§ Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¢Ø®Ø±ÙŠÙ†. \n",
       "\n",
       "ÙƒØ§Ù† Ø³Ù†Ø¨Ù„ ÙŠØªØ¬ÙˆÙ„ ÙÙŠ Ø£Ù†Ø­Ø§Ø¡ Ù…ØµØ±ØŒ ÙŠØ²ÙˆØ± Ù…Ø¯Ù†Ù‡Ø§ ÙˆÙ‚Ø±Ø§Ù‡Ø§ØŒ Ù…Ù† Ø§Ù„Ù‚Ø§Ù‡Ø±Ø© Ø¥Ù„Ù‰ Ø§Ù„Ø¥Ø³ÙƒÙ†Ø¯Ø±ÙŠØ©ØŒ ÙˆÙ…Ù† Ø§Ù„Ø£Ù‚ØµØ± Ø¥Ù„Ù‰ Ø£Ø³ÙˆØ§Ù†. \n",
       "\n",
       "ÙÙŠ Ø±Ø­Ù„ØªÙ‡ØŒ Ø§Ù„ØªÙ‚Ù‰ Ø³Ù†Ø¨Ù„ Ø¨Ø§Ù„Ø¹Ø¯ÙŠØ¯ Ù…Ù† Ø§Ù„Ø£Ø´Ø®Ø§Øµ ÙˆØ§Ù„Ø­ÙŠÙˆØ§Ù†Ø§ØªØŒ Ø¨Ø¹Ø¶Ù‡Ù… ÙƒØ§Ù† Ù„Ø·ÙŠÙÙ‹Ø§ ÙˆÙˆØ¯ÙˆØ¯Ù‹Ø§ØŒ ÙˆØ§Ù„Ø¨Ø¹Ø¶ Ø§Ù„Ø¢Ø®Ø± ÙƒØ§Ù† Ø¹Ø¯ÙˆØ§Ù†ÙŠÙ‹Ø§ ÙˆØºÙŠØ± ÙˆØ¯ÙŠ. Ù„ÙƒÙ† Ø³Ù†Ø¨Ù„ Ù„Ù… ÙŠØªØºÙŠØ±ØŒ Ø­Ø§ÙØ¸ Ø¹Ù„Ù‰ Ø·Ø¨ÙŠØ¹ØªÙ‡ Ø§Ù„ÙƒØ±ÙŠÙ…Ø©ØŒ Ù…Ø³ØªØ¹Ø¯Ù‹Ø§ Ø¯Ø§Ø¦Ù…Ù‹Ø§ Ù„Ù„ÙˆÙ‚ÙˆÙ Ø¨Ø¬Ø§Ù†Ø¨ Ø§Ù„Ø¶Ø¹ÙŠÙØŒ ÙˆØ­Ù…Ø§ÙŠØ© Ø§Ù„Ù…Ø¸Ù„ÙˆÙ….\n",
       "\n",
       "Ø°Ø§Øª ÙŠÙˆÙ…ØŒ ÙˆØµÙ„ Ø³Ù†Ø¨Ù„ Ø¥Ù„Ù‰ Ù‚Ø±ÙŠØ© ØµØºÙŠØ±Ø© ÙÙŠ Ø§Ù„ØµØ¹ÙŠØ¯. ÙˆØ¬Ø¯ ÙØªØ§Ø© ØµØºÙŠØ±Ø© ØªØ¨ÙƒÙŠ Ø¨Ø´Ø¯Ø©ØŒ ÙÙ‚Ø¯ ÙÙ‚Ø¯Øª Ù‚Ø·ØªÙ‡Ø§ Ø§Ù„Ù…ÙØ¶Ù„Ø©. \n",
       "\n",
       "ØªÙ‚Ø¯Ù… Ø³Ù†Ø¨Ù„ Ø¥Ù„ÙŠÙ‡Ø§ Ø¨Ù‡Ø¯ÙˆØ¡ ÙˆØ³Ø£Ù„Ù‡Ø§ Ø¹Ù† Ø§Ù„Ø£Ù…Ø±. Ø´Ø±Ø­Øª Ù„Ù‡ Ø§Ù„ÙØªØ§Ø© Ø£Ù†Ù‡Ø§ ÙƒØ§Ù†Øª ØªÙ„Ø¹Ø¨ Ù…Ø¹ Ù‚Ø·ØªÙ‡Ø§ ÙÙŠ Ø§Ù„Ø­Ù‚Ù„ØŒ Ù„ÙƒÙ†Ù‡Ø§ Ø§Ù†Ø²Ù„Ù‚Øª ÙˆØ³Ù‚Ø·ØªØŒ ÙˆØªÙØ±ÙƒØª Ù‚Ø·ØªÙ‡Ø§ ÙˆØ­Ø¯Ù‡Ø§. \n",
       "\n",
       "Ù„Ù… ÙŠØªØ±Ø¯Ø¯ Ø³Ù†Ø¨Ù„ Ù„Ø­Ø¸Ø© ÙˆØ§Ø­Ø¯Ø©ØŒ ÙˆØ¨Ø¯Ø£ ÙŠØ¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù‚Ø·Ø© Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©. \n",
       "\n",
       "Ø¨Ø¹Ø¯ Ø³Ø§Ø¹Ø§Øª Ù…Ù† Ø§Ù„Ø¨Ø­Ø«ØŒ ÙˆØ¬Ø¯ Ø³Ù†Ø¨Ù„ Ø§Ù„Ù‚Ø·Ø© Ù…Ø®ØªØ¨Ø¦Ø© ØªØ­Øª Ø´Ø¬Ø±Ø©. \n",
       "\n",
       "Ø£Ø®Ø° Ø³Ù†Ø¨Ù„ Ø§Ù„Ù‚Ø·Ø© Ø§Ù„ØµØºÙŠØ±Ø© Ø¥Ù„Ù‰ Ø§Ù„ÙØªØ§Ø© Ø§Ù„ØªÙŠ Ø§Ù†Ø¯ÙØ¹Øª Ù„Ø§Ø­ØªØ¶Ø§Ù†Ù‡Ø§.  \n",
       "\n",
       "ØºÙ…Ø±Øª Ø§Ù„ÙØ±Ø­Ø© Ù‚Ù„Ø¨ Ø§Ù„ÙØªØ§Ø©ØŒ ÙˆØ¹Ø¨Ø±Øª Ø¹Ù† Ø´ÙƒØ±Ù‡Ø§ Ù„Ø³Ù†Ø¨Ù„ Ø¨Ø£ÙØ¶Ù„ Ù…Ø§ Ù„Ø¯ÙŠÙ‡Ø§. \n",
       "\n",
       "ÙˆØ§ØµÙ„ Ø³Ù†Ø¨Ù„ Ø±Ø­Ù„ØªÙ‡ Ø¹Ø¨Ø± Ù…ØµØ±ØŒ Ù…ØªØ¬ÙˆÙ„Ù‹Ø§ ÙÙŠ ØµØ­Ø§Ø±ÙŠÙ‡Ø§ØŒ ÙˆØ´ÙˆØ§Ø±Ø¹Ù‡Ø§ØŒ ÙˆØ£Ø²Ù‚ØªÙ‡Ø§ØŒ Ù…Ø­Ø§ÙƒÙŠÙ‹Ø§ Ù‚ØµØµÙ‹Ø§ Ø¹Ù† Ø§Ù„ÙƒØ±Ù… ÙˆØ§Ù„Ù†Ø¨Ù„ØŒ ÙˆÙ†Ø´Ø± Ø§Ù„ÙØ±Ø­ Ø£ÙŠÙ†Ù…Ø§ Ø­Ù„. \n",
       "\n",
       "ÙƒØ§Ù† Ø³Ù†Ø¨Ù„ Ø±Ù…Ø²Ù‹Ø§ Ù„Ù„Ø­Ù†Ø§Ù† ÙˆØ§Ù„Ù‚ÙˆØ©ØŒ Ø¯Ù„ÙŠÙ„Ù‹Ø§ Ø¹Ù„Ù‰ Ø£Ù† Ø§Ù„Ù‚Ù„ÙˆØ¨ Ø§Ù„Ø·ÙŠØ¨Ø© Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ ÙƒÙ„ Ù…ÙƒØ§Ù†ØŒ Ù…Ù‡Ù…Ø§ ÙƒØ§Ù†Øª ØµØ¹ÙˆØ¨Ø© Ø§Ù„Ø­ÙŠØ§Ø©. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash-001',\n",
    "    generation_config=genai.GenerationConfig(\n",
    "        # These are the default values for gemini-1.5-flash-001.\n",
    "        temperature=1.0,\n",
    "        top_k=64,\n",
    "        top_p=0.95,\n",
    "    ))\n",
    "\n",
    "story_prompt = \"Ù‚Ù… Ø¨ÙƒØªØ§Ø¨Ù‡ Ù‚ØµÙ‡ Ø¹Ù† Ù‚Ø· Ø£Ø¨ÙŠØ¶ Ø°Ùˆ Ø´Ø®ØµÙŠÙ‡ Ù‚ÙˆÙŠÙ‡ ÙˆÙ‚Ù„Ø¨ Ø·ÙŠØ¨ ÙŠØªØ¬ÙˆÙ„ ÙÙŠ Ø§Ù†Ø­Ø§Ø¡ Ù…ØµØ±\"\n",
    "response = model.generate_content(story_prompt)\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rMrYs1koY6DX"
   },
   "source": [
    "---\n",
    "## Prompting\n",
    "---\n",
    "This section contains some prompts from the chapter for you to try out directly in the API. Try changing the text here to see how each prompt performs with different instructions, more examples, or any other changes you can think of."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhj_tQidZJP7"
   },
   "source": [
    "### 1. Zero-shot\n",
    "\n",
    "Zero-shot prompts are prompts that describe the request for the model directly.\n",
    "\n",
    "<table align=left>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://aistudio.google.com/prompts/1gzKKgDHwkAvexG5Up0LMtl1-6jKMKe4g\"><img src=\"https://ai.google.dev/site-assets/images/marketing/home/icon-ais.png\" style=\"height: 24px\" height=24/> Open in AI Studio</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T13:24:25.262625Z",
     "iopub.status.busy": "2024-11-11T13:24:25.261864Z",
     "iopub.status.idle": "2024-11-11T13:24:25.522715Z",
     "shell.execute_reply": "2024-11-11T13:24:25.521596Z",
     "shell.execute_reply.started": "2024-11-11T13:24:25.262583Z"
    },
    "id": "1_t-cwnDZzbH",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Sentiment: **POSITIVE**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configs = genai.GenerationConfig(\n",
    "        temperature=0.1,\n",
    "        top_p=1,\n",
    "        max_output_tokens=5)\n",
    "\n",
    "model = genai.GenerativeModel('gemini-1.5-flash-001', generation_config=configs)\n",
    "\n",
    "zero_shot_prompt = \"\"\"Classify movie reviews as POSITIVE, NEUTRAL or NEGATIVE.\n",
    "Review: \"Her\" is a disturbing study revealing the direction\n",
    "humanity is headed if AI is allowed to keep evolving,\n",
    "unchecked. I wish there were more movies like this masterpiece.\n",
    "Sentiment: \"\"\"\n",
    "\n",
    "response = model.generate_content(zero_shot_prompt)\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3b5568bdeb11"
   },
   "source": [
    "#### Enum mode\n",
    "\n",
    "The models are trained to generate text, and can sometimes produce more text than you may wish for. In the preceding example, the model will output the label, sometimes it can include a preceding \"Sentiment\" label, and without an output token limit, it may also add explanatory text afterwards.\n",
    "\n",
    "The Gemini API has an [Enum mode](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Enum.ipynb) feature that allows you to constrain the output to a fixed set of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T13:24:32.067597Z",
     "iopub.status.busy": "2024-11-11T13:24:32.066883Z",
     "iopub.status.idle": "2024-11-11T13:24:32.711317Z",
     "shell.execute_reply": "2024-11-11T13:24:32.710065Z",
     "shell.execute_reply.started": "2024-11-11T13:24:32.067553Z"
    },
    "id": "ad118a56c598",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import enum\n",
    "\n",
    "class Sentiment(enum.Enum):\n",
    "    # The response should be one of the following options    \n",
    "    POSITIVE = \"positive\"\n",
    "    NEUTRAL = \"neutral\"\n",
    "    NEGATIVE = \"negative\"\n",
    "\n",
    "configs = genai.GenerationConfig(\n",
    "    response_mime_type=\"text/x.enum\",\n",
    "    response_schema=Sentiment\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "positive"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = genai.GenerativeModel(\n",
    "            'gemini-1.5-flash-001',\n",
    "            generation_config=configs)\n",
    "\n",
    "response = model.generate_content(zero_shot_prompt)\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0udiSwNbv45W"
   },
   "source": [
    "### 2. One-shot and few-shot\n",
    "\n",
    "Providing an example of the expected response is known as a \"one-shot\" prompt. When you provide multiple examples, it is a \"few-shot\" prompt.\n",
    "\n",
    "<table align=left>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://aistudio.google.com/prompts/1jjWkjUSoMXmLvMJ7IzADr_GxHPJVV2bg\"><img src=\"https://ai.google.dev/site-assets/images/marketing/home/icon-ais.png\" style=\"height: 24px\" height=24/> Open in AI Studio</a>\n",
    "  </td>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T13:24:38.971558Z",
     "iopub.status.busy": "2024-11-11T13:24:38.971146Z",
     "iopub.status.idle": "2024-11-11T13:24:39.319086Z",
     "shell.execute_reply": "2024-11-11T13:24:39.317958Z",
     "shell.execute_reply.started": "2024-11-11T13:24:38.971519Z"
    },
    "id": "hd4mVUukwOKZ",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "\"size\": \"large\",\n",
      "\"type\": \"normal\",\n",
      "\"ingredients\": [\"cheese\", \"pineapple\"]\n",
      "}\n",
      "``` \n",
      "\n"
     ]
    }
   ],
   "source": [
    "configs = genai.GenerationConfig(\n",
    "        temperature=0.1,\n",
    "        top_p=1,\n",
    "        max_output_tokens=250,\n",
    "    )\n",
    "\n",
    "model = genai.GenerativeModel('gemini-1.5-flash-latest', generation_config=configs)\n",
    "\n",
    "few_shot_prompt = \"\"\"Parse a customer's pizza order into valid JSON:\n",
    "\n",
    "EXAMPLE:\n",
    "I want a small pizza with cheese, tomato sauce, and pepperoni.\n",
    "JSON Response:\n",
    "```\n",
    "{\n",
    "\"size\": \"small\",\n",
    "\"type\": \"normal\",\n",
    "\"ingredients\": [\"cheese\", \"tomato sauce\", \"peperoni\"]\n",
    "}\n",
    "```\n",
    "\n",
    "EXAMPLE:\n",
    "Can I get a large pizza with tomato sauce, basil and mozzarella\n",
    "JSON Response:\n",
    "```\n",
    "{\n",
    "\"size\": \"large\",\n",
    "\"type\": \"normal\",\n",
    "\"ingredients\": [\"tomato sauce\", \"basil\", \"mozzarella\"]\n",
    "}\n",
    "\n",
    "ORDER:\n",
    "\"\"\"\n",
    "\n",
    "customer_order = \"Give me a large with cheese & pineapple\"\n",
    "\n",
    "response = model.generate_content([few_shot_prompt, customer_order])\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "021293096f08"
   },
   "source": [
    "#### JSON mode\n",
    "\n",
    "To provide control over the schema, and to ensure that you only receive JSON (with no other text or markdown), you can use the Gemini API's [JSON mode](https://github.com/google-gemini/cookbook/blob/main/quickstarts/JSON_mode.ipynb). This forces the model to constrain decoding, such that token selection is guided by the supplied schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T08:14:58.319116Z",
     "iopub.status.busy": "2024-11-11T08:14:58.318683Z",
     "iopub.status.idle": "2024-11-11T08:14:58.741279Z",
     "shell.execute_reply": "2024-11-11T08:14:58.739848Z",
     "shell.execute_reply.started": "2024-11-11T08:14:58.319073Z"
    },
    "id": "50fbf0260912",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import typing_extensions as typing\n",
    "\n",
    "class PizzaOrder(typing.TypedDict):\n",
    "    size: str\n",
    "    ingredients: list[str]\n",
    "    type: str\n",
    "\n",
    "configs = genai.GenerationConfig(\n",
    "        temperature=0.1,\n",
    "        response_mime_type=\"application/json\",\n",
    "        response_schema=PizzaOrder,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"ingredients\": [\"apple\", \"chocolate\"], \"size\": \"large\", \"type\": \"dessert\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = genai.GenerativeModel('gemini-1.5-flash-latest', generation_config=configs)\n",
    "\n",
    "response = model.generate_content(\"Can I have a large dessert pizza with apple and chocolate\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4a93e338e57c"
   },
   "source": [
    "### 3. Chain of Thought (CoT)\n",
    "\n",
    "Direct prompting on LLMs can return answers quickly and (in terms of output token usage) efficiently, but they can be prone to hallucination. The answer may \"look\" correct (in terms of language and syntax) but is incorrect in terms of factuality and reasoning.\n",
    "\n",
    "Chain-of-Thought prompting is a technique where you instruct the model to output intermediate reasoning steps, and it typically gets better results, especially when combined with few-shot examples. It is worth noting that this technique doesn't completely eliminate hallucinations, and that it tends to cost more to run, due to the increased token count.\n",
    "\n",
    "As models like the Gemini family are trained to be \"chatty\" and provide reasoning steps, you can ask the model to be more direct in the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T13:24:47.261345Z",
     "iopub.status.busy": "2024-11-11T13:24:47.260232Z",
     "iopub.status.idle": "2024-11-11T13:24:47.553694Z",
     "shell.execute_reply": "2024-11-11T13:24:47.552589Z",
     "shell.execute_reply.started": "2024-11-11T13:24:47.261287Z"
    },
    "id": "5715555db1c1",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "52 \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now, I\n",
    "am 20 years old. How old is my partner? Return the answer immediately.\"\"\"\n",
    "\n",
    "model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
    "response = model.generate_content(prompt)\n",
    "\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e12b19677bfd"
   },
   "source": [
    "Now try the same approach, but indicate to the model that it should \"think step by step\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T13:24:50.614301Z",
     "iopub.status.busy": "2024-11-11T13:24:50.613897Z",
     "iopub.status.idle": "2024-11-11T13:24:51.230860Z",
     "shell.execute_reply": "2024-11-11T13:24:51.229420Z",
     "shell.execute_reply.started": "2024-11-11T13:24:50.614263Z"
    },
    "id": "ffd7536a481f",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here's how to solve this:\n",
       "\n",
       "* **When you were 4:** Your partner was 3 times your age, meaning they were 4 * 3 = 12 years old.\n",
       "* **Age difference:** The age difference between you and your partner is 12 - 4 = 8 years.\n",
       "* **Your current age:** You are now 20 years old.\n",
       "* **Partner's current age:** Since the age difference remains constant, your partner is 20 + 8 = **28 years old**. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now,\n",
    "I am 20 years old. How old is my partner? Let's think step by step.\"\"\"\n",
    "\n",
    "response = model.generate_content(prompt)\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oiLgBQJj0V53"
   },
   "source": [
    "### 4. ReAct: Reason and act\n",
    "\n",
    "In this example you will run a ReAct prompt directly in the Gemini API and perform the searching steps yourself. As this prompt follows a well-defined structure, there are frameworks available that wrap the prompt into easier-to-use APIs that make tool calls automatically, such as the LangChain example from the chapter.\n",
    "\n",
    "To try this out with the Wikipedia search engine, check out the [Searching Wikipedia with ReAct](https://github.com/google-gemini/cookbook/blob/main/examples/Search_Wikipedia_using_ReAct.ipynb) cookbook example.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T13:24:54.954286Z",
     "iopub.status.busy": "2024-11-11T13:24:54.953228Z",
     "iopub.status.idle": "2024-11-11T13:24:54.961300Z",
     "shell.execute_reply": "2024-11-11T13:24:54.960188Z",
     "shell.execute_reply.started": "2024-11-11T13:24:54.954242Z"
    },
    "id": "cBgyNJ5z0VSs",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_instructions = \"\"\"\n",
    "Solve a question answering task with interleaving Thought, Action, Observation steps. Thought can reason about the current situation,\n",
    "Observation is understanding relevant information from an Action's output and Action can be one of three types:\n",
    " (1) <search>entity</search>, which searches the exact entity on Wikipedia and returns the first paragraph if it exists. If not, it\n",
    "     will return some similar entities to search and you can try to search the information from those topics.\n",
    " (2) <lookup>keyword</lookup>, which returns the next sentence containing keyword in the current context. This only does exact matches,\n",
    "     so keep your searches short.\n",
    " (3) <finish>answer</finish>, which returns the answer and finishes the task.\n",
    "\"\"\"\n",
    "\n",
    "example1 = \"\"\"Question\n",
    "Ù…Ù† Ù‡Ùˆ Ù…Ø¤Ø³Ø³ Ù…Ø¬Ù„Ø© \"Ø§Ù„Ø£Ù‡Ø±Ø§Ù…\" Ø§Ù„Ù…ØµØ±ÙŠØ©ØŸ\n",
    "\n",
    "Thought 1\n",
    "Ø§Ù„Ø³Ø¤Ø§Ù„ ÙŠØ·Ù„Ø¨ Ù…Ø¹Ø±ÙØ© Ù…Ù† Ù‡Ùˆ Ù…Ø¤Ø³Ø³ Ù…Ø¬Ù„Ø© \"Ø§Ù„Ø£Ù‡Ø±Ø§Ù…\" Ø§Ù„Ù…ØµØ±ÙŠØ©. Ø³Ø£Ø¨Ø­Ø« Ø¹Ù† \"Ù…Ø¤Ø³Ø³ Ù…Ø¬Ù„Ø© Ø§Ù„Ø£Ù‡Ø±Ø§Ù…\" Ø¹Ù„Ù‰ ÙˆÙŠÙƒÙŠØ¨ÙŠØ¯ÙŠØ§.\n",
    "\n",
    "Action 1\n",
    "<search>Ù…Ø¤Ø³Ø³ Ù…Ø¬Ù„Ø© Ø§Ù„Ø£Ù‡Ø±Ø§Ù…</search>\n",
    "\n",
    "Observation 1\n",
    "Ù…Ø¤Ø³Ø³ Ù…Ø¬Ù„Ø© Ø§Ù„Ø£Ù‡Ø±Ø§Ù… Ù‡Ùˆ Ø§Ù„ØµØ­ÙÙŠ Ø§Ù„Ù…ØµØ±ÙŠ Ù…Ø­Ù…Ø¯ Ø¹Ù„ÙŠ.\n",
    "\n",
    "Thought 2\n",
    "Ø§Ù„ÙÙ‚Ø±Ø© ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©. Ù…Ø­Ù…Ø¯ Ø¹Ù„ÙŠ Ù‡Ùˆ Ù…Ø¤Ø³Ø³ Ù…Ø¬Ù„Ø© Ø§Ù„Ø£Ù‡Ø±Ø§Ù….\n",
    "\n",
    "Action 2\n",
    "<finish>Ù…Ø­Ù…Ø¯ Ø¹Ù„ÙŠ</finish>\n",
    "\"\"\"\n",
    "\n",
    "example2 = \"\"\"Question\n",
    "Ù…Ù† Ù‡Ùˆ Ø§Ù„Ø´Ø§Ø¹Ø± Ø§Ù„Ù…ØµØ±ÙŠ Ø§Ù„Ø°ÙŠ ÙƒØªØ¨ Ù‚ØµÙŠØ¯Ø© \"Ø§Ù„Ø£Ø·Ù„Ø§Ù„\"ØŸ\n",
    "\n",
    "Thought 1\n",
    "Ø§Ù„Ø³Ø¤Ø§Ù„ ÙŠØ·Ù„Ø¨ Ù…Ø¹Ø±ÙØ© Ù…Ù† Ù‡Ùˆ Ø§Ù„Ø´Ø§Ø¹Ø± Ø§Ù„Ù…ØµØ±ÙŠ Ø§Ù„Ø°ÙŠ ÙƒØªØ¨ Ù‚ØµÙŠØ¯Ø© \"Ø§Ù„Ø£Ø·Ù„Ø§Ù„\". Ø³Ø£Ø¨Ø­Ø« Ø¹Ù† \"Ø§Ù„Ø´Ø§Ø¹Ø± Ø§Ù„Ø°ÙŠ ÙƒØªØ¨ Ù‚ØµÙŠØ¯Ø© Ø§Ù„Ø£Ø·Ù„Ø§Ù„\" ÙÙŠ ÙˆÙŠÙƒÙŠØ¨ÙŠØ¯ÙŠØ§.\n",
    "\n",
    "Action 1\n",
    "<search>Ù‚ØµÙŠØ¯Ø© Ø§Ù„Ø£Ø·Ù„Ø§Ù„</search>\n",
    "\n",
    "Observation 1\n",
    "Ù‚ØµÙŠØ¯Ø© \"Ø§Ù„Ø£Ø·Ù„Ø§Ù„\" ÙƒØªØ¨Ù‡Ø§ Ø§Ù„Ø´Ø§Ø¹Ø± Ø§Ù„Ù…ØµØ±ÙŠ Ø¥Ø¨Ø±Ø§Ù‡ÙŠÙ… Ù†Ø§Ø¬ÙŠ.\n",
    "\n",
    "Thought 2\n",
    "Ø§Ù„ÙÙ‚Ø±Ø© ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©. Ø¥Ø¨Ø±Ø§Ù‡ÙŠÙ… Ù†Ø§Ø¬ÙŠ Ù‡Ùˆ Ø§Ù„Ø´Ø§Ø¹Ø± Ø§Ù„Ø°ÙŠ ÙƒØªØ¨ Ù‚ØµÙŠØ¯Ø© \"Ø§Ù„Ø£Ø·Ù„Ø§Ù„\".\n",
    "\n",
    "Action 2\n",
    "<finish>Ø¥Ø¨Ø±Ø§Ù‡ÙŠÙ… Ù†Ø§Ø¬ÙŠ</finish>\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"Question\n",
    "Ù…Ø§ Ù‡ÙŠ Ø£Ø´Ù‡Ø± Ù…Ø¹Ø§Ù„Ù… Ø§Ù„Ø³ÙŠØ§Ø­Ø© ÙÙŠ Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø£Ù‚ØµØ±ØŸ\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C3wbfstjTgey"
   },
   "source": [
    "To capture a single step at a time, while ignoring any hallucinated Observation steps, you will use `stop_sequences` to end the generation process. The steps are `Thought`, `Action`, `Observation`, in that order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T13:25:01.584042Z",
     "iopub.status.busy": "2024-11-11T13:25:01.583464Z",
     "iopub.status.idle": "2024-11-11T13:25:03.683905Z",
     "shell.execute_reply": "2024-11-11T13:25:03.682672Z",
     "shell.execute_reply.started": "2024-11-11T13:25:01.583948Z"
    },
    "id": "8mxrXRkRTdXm",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought 1\n",
      "Ø§Ù„Ø³Ø¤Ø§Ù„ ÙŠØ·Ù„Ø¨ Ù…Ø¹Ø±ÙØ© Ø£Ø´Ù‡Ø± Ù…Ø¹Ø§Ù„Ù… Ø§Ù„Ø³ÙŠØ§Ø­Ø© ÙÙŠ Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø£Ù‚ØµØ±. Ø³Ø£Ø¨Ø­Ø« Ø¹Ù† \"Ù…Ø¹Ø§Ù„Ù… Ø§Ù„Ø³ÙŠØ§Ø­Ø© ÙÙŠ Ø§Ù„Ø£Ù‚ØµØ±\" ÙÙŠ ÙˆÙŠÙƒÙŠØ¨ÙŠØ¯ÙŠØ§.\n",
      "\n",
      "Action 1\n",
      "<search>Ù…Ø¹Ø§Ù„Ù… Ø§Ù„Ø³ÙŠØ§Ø­Ø© ÙÙŠ Ø§Ù„Ø£Ù‚ØµØ±</search>\n",
      "\n",
      "Observation 1\n",
      "ØªÙØ¹Ø±Ù Ø§Ù„Ø£Ù‚ØµØ± Ø¨ÙƒÙˆÙ†Ù‡Ø§ \"Ù…ØªØ­ÙÙ‹Ø§ Ù…ÙØªÙˆØ­Ù‹Ø§\" Ø¨Ø³Ø¨Ø¨ ÙƒØ«Ø±Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ù… Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠØ© ÙˆØ§Ù„Ø¢Ø«Ø§Ø± Ø§Ù„ÙØ±Ø¹ÙˆÙ†ÙŠØ© ÙÙŠÙ‡Ø§. \n",
      "\n",
      "Thought 2\n",
      "Ø§Ù„ÙÙ‚Ø±Ø© Ù„Ø§ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ø­Ø¯Ø¯Ø© Ø¹Ù† Ø£Ø´Ù‡Ø± Ù…Ø¹Ø§Ù„Ù… Ø§Ù„Ø³ÙŠØ§Ø­Ø©ØŒ Ø³Ø£Ø¨Ø­Ø« Ø¹Ù† \"Ø£Ø´Ù‡Ø± Ù…Ø¹Ø§Ù„Ù… Ø§Ù„Ø³ÙŠØ§Ø­Ø© ÙÙŠ Ø§Ù„Ø£Ù‚ØµØ±\" ÙÙŠ ÙˆÙŠÙƒÙŠØ¨ÙŠØ¯ÙŠØ§.\n",
      "\n",
      "Action 2\n",
      "<search>Ø£Ø´Ù‡Ø± Ù…Ø¹Ø§Ù„Ù… Ø§Ù„Ø³ÙŠØ§Ø­Ø© ÙÙŠ Ø§Ù„Ø£Ù‚ØµØ±</search>\n",
      "\n",
      "Observation 2\n",
      "Ù…Ù† Ø£Ø´Ù‡Ø± Ù…Ø¹Ø§Ù„Ù… Ø§Ù„Ø£Ù‚ØµØ±: \n",
      "\n",
      "Thought 3\n",
      "Ø§Ù„ÙÙ‚Ø±Ø© ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù‚Ø§Ø¦Ù…Ø© Ø¨Ø¨Ø¹Ø¶ Ø£Ø´Ù‡Ø± Ø§Ù„Ù…Ø¹Ø§Ù„Ù… Ø§Ù„Ø³ÙŠØ§Ø­ÙŠØ© ÙÙŠ Ø§Ù„Ø£Ù‚ØµØ±.\n",
      "\n",
      "Action 3\n",
      "<finish>Ù…Ù† Ø£Ø´Ù‡Ø± Ù…Ø¹Ø§Ù„Ù… Ø§Ù„Ø£Ù‚ØµØ±: Ù…Ø¹Ø¨Ø¯ Ø§Ù„Ø£Ù‚ØµØ±ØŒ Ù…Ø¹Ø¨Ø¯ Ø§Ù„ÙƒØ±Ù†ÙƒØŒ ÙˆØ§Ø¯ÙŠ Ø§Ù„Ù…Ù„ÙˆÙƒØŒ ÙˆØ§Ø¯ÙŠ Ø§Ù„Ù…Ù„ÙƒØ§ØªØŒ ÙˆÙ‚Ø±ÙŠØ© Ø§Ù„ØºØ±Ù†Ù‚Ø©ØŒ ÙˆÙ…Ø¹Ø¨Ø¯ Ø§Ù„Ø£Ù‚ØµØ±ØŒ ÙˆÙ…Ø¹Ø¨Ø¯ Ø§Ù„Ø£Ù‚ØµØ±.</finish> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
    "react_chat = model.start_chat()\n",
    "\n",
    "# You will perform the Action, so generate up to, but not including, the Observation.\n",
    "config = genai.GenerationConfig()\n",
    "\n",
    "resp = react_chat.send_message(\n",
    "                    [model_instructions, example1, example2, question],\n",
    "                    generation_config=config)\n",
    "\n",
    "print(resp.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bo0tzf4nX6dA"
   },
   "source": [
    "This process repeats until the `<finish>` action is reached. You can continue running this yourself if you like, or try the [Wikipedia example](https://github.com/google-gemini/cookbook/blob/main/examples/Search_Wikipedia_using_ReAct.ipynb) to see a fully automated ReAct system at work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jPiZ_eIIaVPt"
   },
   "source": [
    "---\n",
    "## Code prompting\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZinKamwXeR6C"
   },
   "source": [
    "### 1. Generating code\n",
    "\n",
    "The Gemini family of models can be used to generate code, configuration and scripts. Generating code can be helpful when learning to code, learning a new language or for rapidly generating a first draft.\n",
    "\n",
    "It's important to be aware that since LLMs can't reason, and can repeat training data, it's essential to read and test your code first, and comply with any relevant licenses.\n",
    "\n",
    "<table align=left>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://aistudio.google.com/prompts/1YX71JGtzDjXQkgdes8bP6i3oH5lCRKxv\"><img src=\"https://ai.google.dev/site-assets/images/marketing/home/icon-ais.png\" style=\"height: 24px\" height=24/> Open in AI Studio</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T13:25:12.551884Z",
     "iopub.status.busy": "2024-11-11T13:25:12.550944Z",
     "iopub.status.idle": "2024-11-11T13:25:12.877757Z",
     "shell.execute_reply": "2024-11-11T13:25:12.876668Z",
     "shell.execute_reply.started": "2024-11-11T13:25:12.551839Z"
    },
    "id": "fOQP9pqmeUO1",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "def factorial(n):\n",
       "  if n == 0:\n",
       "    return 1\n",
       "  else:\n",
       "    return n * factorial(n-1)\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash-latest',\n",
    "    generation_config=genai.GenerationConfig(\n",
    "        temperature=1,\n",
    "        top_p=1,\n",
    "        max_output_tokens=1024,\n",
    "    ))\n",
    "\n",
    "# Gemini 1.5 models are very chatty, so it helps to specify they stick to the code.\n",
    "code_prompt = \"\"\"\n",
    "Write a Python function to calculate the factorial of a number. No explanation, provide only the code.\n",
    "\"\"\"\n",
    "\n",
    "response = model.generate_content(code_prompt)\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wlBMWSFhgVRQ"
   },
   "source": [
    "### 2. Code execution\n",
    "\n",
    "The Gemini API can automatically run generated code too, and will return the output.\n",
    "\n",
    "<table align=left>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://aistudio.google.com/prompts/11veFr_VYEwBWcLkhNLr-maCG0G8sS_7Z\"><img src=\"https://ai.google.dev/site-assets/images/marketing/home/icon-ais.png\" style=\"height: 24px\" height=24/> Open in AI Studio</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T13:25:20.197695Z",
     "iopub.status.busy": "2024-11-11T13:25:20.196901Z",
     "iopub.status.idle": "2024-11-11T13:25:22.594758Z",
     "shell.execute_reply": "2024-11-11T13:25:22.593755Z",
     "shell.execute_reply.started": "2024-11-11T13:25:20.197650Z"
    },
    "id": "jT3OfWYfhjRL",
    "trusted": true
   },
   "outputs": [
    {
     "ename": "InternalServerError",
     "evalue": "500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalServerError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m genai\u001b[38;5;241m.\u001b[39mGenerativeModel(\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgemini-1.5-flash-latest\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      3\u001b[0m     tools\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcode_execution\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m code_exec_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124mCalculate the sum of the first 14 prime numbers. Only consider the odd primes, and make sure you get them all.\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m----> 9\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode_exec_prompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m Markdown(response\u001b[38;5;241m.\u001b[39mtext)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp-env/lib/python3.9/site-packages/google/generativeai/generative_models.py:331\u001b[0m, in \u001b[0;36mGenerativeModel.generate_content\u001b[0;34m(self, contents, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation_types\u001b[38;5;241m.\u001b[39mGenerateContentResponse\u001b[38;5;241m.\u001b[39mfrom_iterator(iterator)\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrequest_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    335\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation_types\u001b[38;5;241m.\u001b[39mGenerateContentResponse\u001b[38;5;241m.\u001b[39mfrom_response(response)\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m google\u001b[38;5;241m.\u001b[39mapi_core\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mInvalidArgument \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp-env/lib/python3.9/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py:830\u001b[0m, in \u001b[0;36mGenerativeServiceClient.generate_content\u001b[0;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_universe_domain()\n\u001b[1;32m    829\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[0;32m--> 830\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[1;32m    838\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp-env/lib/python3.9/site-packages/google/api_core/gapic_v1/method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp-env/lib/python3.9/site-packages/google/api_core/retry/retry_unary.py:293\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    290\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[1;32m    292\u001b[0m )\n\u001b[0;32m--> 293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp-env/lib/python3.9/site-packages/google/api_core/retry/retry_unary.py:153\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m     \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43msleep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(sleep)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp-env/lib/python3.9/site-packages/google/api_core/retry/retry_base.py:212\u001b[0m, in \u001b[0;36m_retry_error_helper\u001b[0;34m(exc, deadline, next_sleep, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m predicate_fn(exc):\n\u001b[1;32m    207\u001b[0m     final_exc, source_exc \u001b[38;5;241m=\u001b[39m exc_factory_fn(\n\u001b[1;32m    208\u001b[0m         error_list,\n\u001b[1;32m    209\u001b[0m         RetryFailureReason\u001b[38;5;241m.\u001b[39mNON_RETRYABLE_ERROR,\n\u001b[1;32m    210\u001b[0m         original_timeout,\n\u001b[1;32m    211\u001b[0m     )\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msource_exc\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    214\u001b[0m     on_error_fn(exc)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp-env/lib/python3.9/site-packages/google/api_core/retry/retry_unary.py:144\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sleep \u001b[38;5;129;01min\u001b[39;00m sleep_generator:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 144\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n\u001b[1;32m    146\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(_ASYNC_RETRY_WARNING)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp-env/lib/python3.9/site-packages/google/api_core/timeout.py:120\u001b[0m, in \u001b[0;36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# Avoid setting negative timeout\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout \u001b[38;5;241m-\u001b[39m time_since_first_attempt)\n\u001b[0;32m--> 120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp-env/lib/python3.9/site-packages/google/api_core/grpc_helpers.py:78\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m callable_(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[0;31mInternalServerError\u001b[0m: 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting"
     ]
    }
   ],
   "source": [
    "model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash-latest',\n",
    "    tools='code_execution')\n",
    "\n",
    "code_exec_prompt = \"\"\"\n",
    "Calculate the sum of the first 14 prime numbers. Only consider the odd primes, and make sure you get them all.\n",
    "\"\"\"\n",
    "\n",
    "response = model.generate_content(code_exec_prompt)\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZspT1GSkjG6d"
   },
   "source": [
    "While this looks like a single-part response, you can inspect the response to see the each of the steps: initial text, code generation, execution results, and final text summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T13:25:33.666437Z",
     "iopub.status.busy": "2024-11-11T13:25:33.664999Z",
     "iopub.status.idle": "2024-11-11T13:25:33.672514Z",
     "shell.execute_reply": "2024-11-11T13:25:33.671251Z",
     "shell.execute_reply.started": "2024-11-11T13:25:33.666374Z"
    },
    "id": "j4gQVzcRjRX-",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: \"I will calculate the sum of the first 14 odd prime numbers.\\n\\nFirst, I will define a function to check if a number is prime:\\n\\n\"\n",
      "\n",
      "-----\n",
      "executable_code {\n",
      "  language: PYTHON\n",
      "  code: \"\\ndef is_prime(n):\\n    \\\"\\\"\\\"\\n    Check if a number is prime.\\n    \\\"\\\"\\\"\\n    if n <= 1:\\n        return False\\n    for i in range(2, int(n**0.5) + 1):\\n        if n % i == 0:\\n            return False\\n    return True\\n\"\n",
      "}\n",
      "\n",
      "-----\n",
      "code_execution_result {\n",
      "  outcome: OUTCOME_OK\n",
      "}\n",
      "\n",
      "-----\n",
      "text: \"Now I will loop through the odd numbers, and check if they are prime. I will stop once I find 14 prime numbers.\\n\\n\"\n",
      "\n",
      "-----\n",
      "executable_code {\n",
      "  language: PYTHON\n",
      "  code: \"\\nprimes = []\\nn = 1\\ncount = 0\\nwhile count < 14:\\n    if n % 2 != 0 and is_prime(n):\\n        primes.append(n)\\n        count += 1\\n    n += 2\\n\\nprint(f\\'The first 14 odd primes are: {primes}\\')\\nprint(f\\'The sum of the first 14 odd primes is: {sum(primes)}\\')\\n\"\n",
      "}\n",
      "\n",
      "-----\n",
      "code_execution_result {\n",
      "  outcome: OUTCOME_OK\n",
      "  output: \"The first 14 odd primes are: [3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47]\\nThe sum of the first 14 odd primes is: 326\\n\"\n",
      "}\n",
      "\n",
      "-----\n",
      "text: \"Therefore, the sum of the first 14 odd prime numbers is **326**. \\n\"\n",
      "\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "for part in response.candidates[0].content.parts:\n",
    "  print(part)\n",
    "  print(\"-----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1gUX8QzCj4d5"
   },
   "source": [
    "### 3. Explaining code\n",
    "\n",
    "The Gemini family of models can explain code to you too.\n",
    "\n",
    "<table align=left>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://aistudio.google.com/prompts/1N7LGzWzCYieyOf_7bAG4plrmkpDNmUyb\"><img src=\"https://ai.google.dev/site-assets/images/marketing/home/icon-ais.png\" style=\"height: 24px\" height=24/> Open in AI Studio</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T13:25:38.852525Z",
     "iopub.status.busy": "2024-11-11T13:25:38.852076Z",
     "iopub.status.idle": "2024-11-11T13:25:41.908425Z",
     "shell.execute_reply": "2024-11-11T13:25:41.907267Z",
     "shell.execute_reply.started": "2024-11-11T13:25:38.852484Z"
    },
    "id": "7_jPMMoxkIEb",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "This code snippet defines a Python class named `GitHubDataCollector` that utilizes the GitHub API to gather and analyze data related to GitHub users and repositories, with a focus on Egyptian contributors. \n",
       "\n",
       "Here's a breakdown of what the code does at a high level:\n",
       "\n",
       "**What it is:**\n",
       "\n",
       "* **GitHub Data Scraping:** It's a script designed to extract data from GitHub using its API. \n",
       "* **Focus on Egypt:**  The code specifically targets contributors and repositories from Egypt, making it useful for analyzing the presence of Egyptian developers on the platform. \n",
       "* **Multiple Functions:** It defines functions to:\n",
       "    * Scrape Egyptian users (users from Egypt)\n",
       "    * Scrape repositories owned by Egyptian users\n",
       "    * Scrape top worldwide repositories (excluding those in Egypt)\n",
       "    * Extract Egyptian contributors from top worldwide repositories\n",
       "\n",
       "**Why you would use it:**\n",
       "\n",
       "* **Research:** Analyze the participation of Egyptian developers in the global GitHub community, understand their contributions, and the types of repositories they work on.\n",
       "* **Community Building:**  Identify Egyptian developers and their projects, enabling collaboration and networking.\n",
       "* **Talent Sourcing:**  Recruiting organizations might leverage this script to find skilled developers in Egypt.\n",
       "\n",
       "**Key Points:**\n",
       "\n",
       "* **GitHubAPI Class:**  The code inherits from a `GitHubAPI` class, likely a custom class to interact with the GitHub API. \n",
       "* **Logging and Progress:**  Includes `logging` and `tqdm` for logging activities and tracking progress.\n",
       "* **CSV Handling:** The code uses the `csv` module to read and write scraped data to CSV files.\n",
       "\n",
       "In summary, this Python script is a tool for researchers, community builders, or recruiters who want to analyze and understand the presence of Egyptian developers on GitHub. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_contents = !curl https://raw.githubusercontent.com/code-quests/open-source-analysis-notebook/refs/heads/main/src/data_collection.py\n",
    "\n",
    "explain_prompt = f\"\"\"\n",
    "Please explain what this file does at a very high level. Breifly, What is it, and why would I use it?\n",
    "\n",
    "```\n",
    "{file_contents}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
    "\n",
    "response = model.generate_content(explain_prompt)\n",
    "Markdown(response.text)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "day-1-prompting.ipynb",
   "toc_visible": true
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "nlp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
