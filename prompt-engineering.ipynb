{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "csNPnkuCobmG"
   },
   "source": [
    "# PROMPT ENGINEERING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create Conda Envirnoment\n",
    "Here are the steps to create a new Conda environment, set secrets, and install requirements:\n",
    "\n",
    "1. **Create a New Conda Environment**\n",
    "   Open your terminal and run the following command to create a new Conda environment with a specified name (e.g, `myenv` in this case) and Python version (e.g., `python=3.9`):\n",
    "   ```bash\n",
    "   conda create --name myenv python=3.9\n",
    "   ```   \n",
    "   > Replace `myenv` with your preferred environment name.\n",
    "\n",
    "2. **Activate the Conda Environment**\n",
    "   After creating the environment, activate it using:\n",
    "\n",
    "   ```bash\n",
    "   conda activate myenv\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Set up API key (Environment Variables)\n",
    "\n",
    "   If you need to store API keys or other sensitive information as environment variables, you can follow these steps:\n",
    "\n",
    "**[Option 1]: Setting Secrets Temporarily**\n",
    "\n",
    "   Set the environment variable for the current session by running:\n",
    "   \n",
    "   ```bash\n",
    "   export GOOGLE_API_KEY=\"YOUR_API_KEY_HERE\"\n",
    "   ```\n",
    "   > This will be available only in the current terminal session.\n",
    "\n",
    "**[Option 2]: etting Secrets Permanently for Conda Environment**\n",
    "\n",
    "   To make the environment variable available every time the environment is activated, create a file that will automatically set the variable:\n",
    "\n",
    "   A. Create a directory for environment-specific activation scripts (if it doesn't exist already):\n",
    "\n",
    "   ```bash\n",
    "   mkdir -p $CONDA_PREFIX/etc/conda/activate.d\n",
    "   ```\n",
    "\n",
    "   B. Create a script to export the environment variable:\n",
    "\n",
    "   ```bash\n",
    "   echo 'export GOOGLE_API_KEY=\"YOUR_API_KEY_HERE\"' > $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh\n",
    "   ```\n",
    "\n",
    "   > This will ensure that the `GOOGLE_API_KEY` is set whenever you activate the environment.\n",
    "   > Replace \"YOUR_API_KEY_HERE\" with your actual API created from [Google AI Studio API Key](https://aistudio.google.com/app/apikey)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIzaSyBcG57Pmu-JuFrGbyrLIoFw5UaXu68SeNI\n"
     ]
    }
   ],
   "source": [
    "!echo $GOOGLE_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UAjUV3BsvFXQ"
   },
   "source": [
    "### 3. Install Dependencies/Requirements\n",
    "\n",
    "   If you have a `requirements.txt` file with the dependencies, install them by running:\n",
    "\n",
    "   ```bash\n",
    "   pip install -r requirements.txt\n",
    "   ```\n",
    "   If you have an `environment.yml` file, you can install the dependencies by running:\n",
    "\n",
    "   ```bash\n",
    "   conda env update --file environment.yml\n",
    "   ```\n",
    "\n",
    "   > This will create or update the environment with all the dependencies specified in the `environment.yml` file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-generativeai>=0.8.3 in /root/miniconda3/envs/nlp-env/lib/python3.9/site-packages (from -r requirements.txt (line 2)) (0.8.3)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.10 in /root/miniconda3/envs/nlp-env/lib/python3.9/site-packages (from google-generativeai>=0.8.3->-r requirements.txt (line 2)) (0.6.10)\n",
      "Requirement already satisfied: google-api-core in /root/miniconda3/envs/nlp-env/lib/python3.9/site-packages (from google-generativeai>=0.8.3->-r requirements.txt (line 2)) (2.23.0)\n",
      "Requirement already satisfied: google-api-python-client in /root/miniconda3/envs/nlp-env/lib/python3.9/site-packages (from google-generativeai>=0.8.3->-r requirements.txt (line 2)) (2.153.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in /root/miniconda3/envs/nlp-env/lib/python3.9/site-packages (from google-generativeai>=0.8.3->-r requirements.txt (line 2)) (2.36.0)\n",
      "Requirement already satisfied: protobuf in /root/miniconda3/envs/nlp-env/lib/python3.9/site-packages (from google-generativeai>=0.8.3->-r requirements.txt (line 2)) (5.28.3)\n",
      "Requirement already satisfied: pydantic in /root/miniconda3/envs/nlp-env/lib/python3.9/site-packages (from google-generativeai>=0.8.3->-r requirements.txt (line 2)) (2.9.1)\n",
      "Requirement already satisfied: tqdm in /root/miniconda3/envs/nlp-env/lib/python3.9/site-packages (from google-generativeai>=0.8.3->-r requirements.txt (line 2)) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions in /root/miniconda3/envs/nlp-env/lib/python3.9/site-packages (from google-generativeai>=0.8.3->-r requirements.txt (line 2)) (4.12.2)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /root/miniconda3/envs/nlp-env/lib/python3.9/site-packages (from google-ai-generativelanguage==0.6.10->google-generativeai>=0.8.3->-r requirements.txt (line 2)) (1.25.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /root/miniconda3/envs/nlp-env/lib/python3.9/site-packages (from google-api-core->google-generativeai>=0.8.3->-r requirements.txt (line 2)) (1.66.0)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /root/miniconda3/envs/nlp-env/lib/python3.9/site-packages (from google-api-core->google-generativeai>=0.8.3->-r requirements.txt (line 2)) (2.32.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /root/miniconda3/envs/nlp-env/lib/python3.9/site-packages (from google-auth>=2.15.0->google-generativeai>=0.8.3->-r requirements.txt (line 2)) (5.4.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /root/miniconda3/envs/nlp-env/lib/python3.9/site-packages (from google-auth>=2.15.0->google-generativeai>=0.8.3->-r requirements.txt (line 2)) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /root/miniconda3/envs/nlp-env/lib/python3.9/site-packages (from google-auth>=2.15.0->google-generativeai>=0.8.3->-r requirements.txt (line 2)) (4.9)\n",
      "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /root/miniconda3/envs/nlp-env/lib/python3.9/site-packages (from google-api-python-client->google-generativeai>=0.8.3->-r requirements.txt (line 2)) (0.22.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /root/miniconda3/envs/nlp-env/lib/python3.9/site-packages (from google-api-python-client->google-generativeai>=0.8.3->-r requirements.txt (line 2)) (0.2.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /root/miniconda3/envs/nlp-env/lib/python3.9/site-packages (from google-api-python-client->google-generativeai>=0.8.3->-r requirements.txt (line 2)) (4.1.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /root/miniconda3/envs/nlp-env/lib/python3.9/site-packages (from pydantic->google-generativeai>=0.8.3->-r requirements.txt (line 2)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.3 in /root/miniconda3/envs/nlp-env/lib/python3.9/site-packages (from pydantic->google-generativeai>=0.8.3->-r requirements.txt (line 2)) (2.23.3)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /root/miniconda3/envs/nlp-env/lib/python3.9/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai>=0.8.3->-r requirements.txt (line 2)) (1.67.1)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /root/miniconda3/envs/nlp-env/lib/python3.9/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai>=0.8.3->-r requirements.txt (line 2)) (1.67.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /root/miniconda3/envs/nlp-env/lib/python3.9/site-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai>=0.8.3->-r requirements.txt (line 2)) (3.0.9)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /root/miniconda3/envs/nlp-env/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai>=0.8.3->-r requirements.txt (line 2)) (0.6.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/envs/nlp-env/lib/python3.9/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai>=0.8.3->-r requirements.txt (line 2)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/envs/nlp-env/lib/python3.9/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai>=0.8.3->-r requirements.txt (line 2)) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/envs/nlp-env/lib/python3.9/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai>=0.8.3->-r requirements.txt (line 2)) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/envs/nlp-env/lib/python3.9/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai>=0.8.3->-r requirements.txt (line 2)) (2024.7.4)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "## Get Started\n",
    "-----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dependancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/nlp-env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "from IPython.display import HTML, Markdown, display\n",
    "from tqdm.autonotebook import tqdm as notebook_tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Prompt to Test API\n",
    "\n",
    "In this step, we will test that the API key is set up correctly by making a request to Gemini. The `gemini-1.5-flash` model has been selected here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T13:22:53.163339Z",
     "iopub.status.busy": "2024-11-11T13:22:53.162912Z",
     "iopub.status.idle": "2024-11-11T13:22:54.475578Z",
     "shell.execute_reply": "2024-11-11T13:22:54.474477Z",
     "shell.execute_reply.started": "2024-11-11T13:22:53.163301Z"
    },
    "id": "BV1o0PmcvyJF",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "تخيل أنك تلعب لعبة مع صديقك، لكن هذه اللعبة ليست لعبة عادية! صديقك هو جهاز كمبيوتر، وأنت تُعلم هذا الجهاز كيفية اللعب.\n",
      "\n",
      "الذكاء الاصطناعي يشبه هذا الجهاز الذكي. هو عبارة عن كمبيوتر قوي جدًا يستطيع التعلم منك ومن العالم من حوله مثلما يفعل صديقك. كلما لعبت معه أكثر، كلما تعلم أكثر وأصبح أفضل في اللعب!\n",
      "\n",
      "الآن، بدلاً من لعبة، يستطيع الذكاء الاصطناعي القيام بأشياء أخرى رائعة. مثل ترجمة لغات من لغة لأخرى، أو مساعدتك في البحث عن المعلومات على الإنترنت، أو حتى كتابة قصة! \n",
      "\n",
      "يُشبه الذكاء الاصطناعي  عقلًا ذكيًا يُحاول فهم العالم من حولنا مثلما نفعل نحن البشر. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test = genai.GenerativeModel('gemini-1.5-flash')\n",
    "response = test.generate_content(\"ُاشرحلي الذكاء الاصطناعي وكأني طفل\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f60ed9d8ae41"
   },
   "source": [
    "The response often comes back in markdown format, which you can render directly in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T13:22:57.237239Z",
     "iopub.status.busy": "2024-11-11T13:22:57.236795Z",
     "iopub.status.idle": "2024-11-11T13:22:57.245860Z",
     "shell.execute_reply": "2024-11-11T13:22:57.244718Z",
     "shell.execute_reply.started": "2024-11-11T13:22:57.237199Z"
    },
    "id": "c933e5e460a5",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "تخيل أنك تلعب لعبة مع صديقك، لكن هذه اللعبة ليست لعبة عادية! صديقك هو جهاز كمبيوتر، وأنت تُعلم هذا الجهاز كيفية اللعب.\n",
       "\n",
       "الذكاء الاصطناعي يشبه هذا الجهاز الذكي. هو عبارة عن كمبيوتر قوي جدًا يستطيع التعلم منك ومن العالم من حوله مثلما يفعل صديقك. كلما لعبت معه أكثر، كلما تعلم أكثر وأصبح أفضل في اللعب!\n",
       "\n",
       "الآن، بدلاً من لعبة، يستطيع الذكاء الاصطناعي القيام بأشياء أخرى رائعة. مثل ترجمة لغات من لغة لأخرى، أو مساعدتك في البحث عن المعلومات على الإنترنت، أو حتى كتابة قصة! \n",
       "\n",
       "يُشبه الذكاء الاصطناعي  عقلًا ذكيًا يُحاول فهم العالم من حولنا مثلما نفعل نحن البشر. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "byx0pT9ZMW2Q"
   },
   "source": [
    "\n",
    "### Start a Chat\n",
    "\n",
    "The previous example uses a single-turn, text-in/text-out structure, but you can also set up a multi-turn chat structure too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "أهلا هبة، من دواعي سروري التعرف عليك! ماذا تريدين أن نفعل اليوم؟ \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = test.start_chat(history=[])\n",
    "response = chat.send_message('أهلا! انا هبة')\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T13:23:02.469325Z",
     "iopub.status.busy": "2024-11-11T13:23:02.468569Z",
     "iopub.status.idle": "2024-11-11T13:23:03.330547Z",
     "shell.execute_reply": "2024-11-11T13:23:03.329470Z",
     "shell.execute_reply.started": "2024-11-11T13:23:02.469279Z"
    },
    "id": "7b0372c3c64a",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "يا هبة، ها أنتِ تحبين الفنون! هذا رائع.  في عالم الذكاء الاصطناعي، هناك الكثير من المساحات الفنية التي يمكنه المشاركة فيها:\n",
       "\n",
       "**1. الموسيقى:** \n",
       "* **توليد الألحان**: يمكن للذكاء الاصطناعي أن يُنشئ ألحانًا جديدة تتناسب مع أنماط موسيقية محددة.\n",
       "* **إنشاء الموسيقى التصويرية**:  يمكنه إنشاء موسيقى تصويرية للأفلام أو الألعاب.\n",
       "* **دمج الأصوات**: يمكنه دمج أصوات مختلفة  لإنشاء أصوات جديدة ومميزة.\n",
       "\n",
       "**2. الرسم والتصميم**:\n",
       "* **الرسومات**:  يمكن للذكاء الاصطناعي أن يرسم لوحات فنية تجريدية أو واقعية.\n",
       "* **التصميم**:  يمكنه مساعدتك في تصميم الملابس أو ديكورات المنازل أو حتى  لوحات إعلانية.\n",
       "* **تحويل الصور**:  يمكنه تحويل الصور إلى أشكال فنية مختلفة، مثل الرسومات بالألوان المائية أو اللوحات الزيتية.\n",
       "\n",
       "**3.  الشعر والكتابة**:\n",
       "* **شعر:**  يمكنه  كتابة قصائد بأشكال وصور مختلفة.\n",
       "* **روايات قصيرة**:  يمكنه إنشاء قصص قصيرة أو حتى روايات كاملة.\n",
       "* **سيناريوهات**:  يمكنه كتابة  سيناريوهات للافلام أو المسرحيات.\n",
       "\n",
       "**4. الفنون الرقمية:**\n",
       "* **التحريك**:  يمكنه إنشاء رسوم متحركة.\n",
       "* **العاب الفيديو**:  يمكنه مساعدتك في  تصميم العاب فيديو.\n",
       "* **العروض الضوئية**:  يمكنه إنشاء عروض ضوئية مبهرة.\n",
       "\n",
       "**5.  الرقص**:\n",
       "* **مشاركة الحركات**:  يمكنه تحليل حركات راقصة وإنتاج حركات جديدة.\n",
       "* **التحكم في  الروبوتات**:  يمكنه  برمجة  الروبوتات  لأداء حركات راقصة.\n",
       "\n",
       "هذه ليست سوى بعض الأمثلة،  العالم مفتوح للخيال والإبداع.  \n",
       "\n",
       "ما نوع الفن الذي يشدك أكثر؟  هل ترغبين في معرفة المزيد عن أحد هذه المجالات؟  \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chat.send_message('قولي اي حاجه فنيه ممكن الذكاء الاصطناعي يشارك فيها لانى بحب الفنون')\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> While you have the `chat` object around, the conversation state persists. Confirm that by asking if it knows my name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T13:23:04.076109Z",
     "iopub.status.busy": "2024-11-11T13:23:04.075694Z",
     "iopub.status.idle": "2024-11-11T13:23:04.572911Z",
     "shell.execute_reply": "2024-11-11T13:23:04.571790Z",
     "shell.execute_reply.started": "2024-11-11T13:23:04.076071Z"
    },
    "id": "d3f9591392a7",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "نعم،  أتذكر اسمك!  أنتِ هبة. \n",
       "\n",
       "هل تريدين مني تذكر شيء آخر؟  \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chat.send_message('فاكر اسمي؟')\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "أوه، هذا سؤال صعب! لستُ متأكدة من جميع اهتماماتك، لكنني أتذكر أنك تحبين الفنون. هل هناك نوع معين من الفنون يستهويكِ أكثر من غيره؟ \n",
       "\n",
       "مثلاً، هل تحبين الرسم، الموسيقى، الكتابة، الرقص، أو ربما فنون أخرى مثل التصوير الفوتوغرافي أو السينما؟ \n",
       "\n",
       "أخبريني أكثر عن اهتماماتك الفنية، لعلني أستطيع أن أُقدم لك اقتراحات مُمتعة. 😊\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chat.send_message(' انا بحب ايه؟')\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose a Model\n",
    "\n",
    "\n",
    "The Gemini API provides access to a number of models from the Gemini model family. Read about the available models and their capabilities on the [model overview page](https://ai.google.dev/gemini-api/docs/models/gemini).\n",
    "\n",
    "In this step you'll use the API to list all of the available models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/chat-bison-001',\n",
       " 'models/text-bison-001',\n",
       " 'models/embedding-gecko-001',\n",
       " 'models/gemini-1.0-pro-latest',\n",
       " 'models/gemini-1.0-pro']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.name for model in genai.list_models())[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [`models.list`](https://ai.google.dev/api/models#method:-models.list) response also returns additional information about the model's capabilities, like the token limits and supported parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(name='models/chat-bison-001',\n",
       "      base_model_id='',\n",
       "      version='001',\n",
       "      display_name='PaLM 2 Chat (Legacy)',\n",
       "      description='A legacy text-only model optimized for chat conversations',\n",
       "      input_token_limit=4096,\n",
       "      output_token_limit=1024,\n",
       "      supported_generation_methods=['generateMessage', 'countMessageTokens'],\n",
       "      temperature=0.25,\n",
       "      max_temperature=None,\n",
       "      top_p=0.95,\n",
       "      top_k=40)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(genai.list_models())[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Explore Generation Parameters\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Output length\n",
    "\n",
    "When generating text with an LLM, the output length affects cost and performance. Generating more tokens increases computation, leading to higher energy consumption, latency, and cost.\n",
    "\n",
    "To stop the model from generating tokens past a limit, you can specify the `max_output_length` parameter when using the Gemini API. Specifying this parameter does not influence the generation of the output tokens, so the output will not become more stylistically or textually succinct, but it will stop generating tokens once the specified length is reached. Prompt engineering may be required to generate a more complete output for your given limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "genai.GenerativeModel(\n",
       "    model_name='models/gemini-1.5-flash',\n",
       "    generation_config={'max_output_tokens': 200},\n",
       "    safety_settings={},\n",
       "    tools=None,\n",
       "    system_instruction=None,\n",
       "    cached_content=None\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Configs = genai.GenerationConfig(max_output_tokens=200)\n",
    "\n",
    "limit_output_model = genai.GenerativeModel(\n",
    "                    'gemini-1.5-flash',\n",
    "                     generation_config=Configs)\n",
    "limit_output_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## إقرأ-تيك: مبادرةٌ تكنولوجيةٌ  لإحياء كتابٍ عربيٍ قديم\n",
       "\n",
       "**المقدمة**\n",
       "\n",
       "في عالمٍ سريعٍ يتهافت فيه الناس على المحتوى الرقمي، غالباً ما يتم إهمال التراث الثقافي والتاريخي،  وتُنسى الكتب القديمة والعلم والمعرفة التي تحتويها. ولتعزيز الوعي بأهمية حفظ التراث العربي، ولتعريف الأجيال الجديدة بالموروث الفكري الذي ورثوه عن أسلافهم،  نشأت مبادرة \"إقرأ-تيك\"  من خلال موقعها الإلكتروني (eqraatech.com).  \n",
       "\n",
       "**هدف \"إقرأ-تيك\" **\n",
       "\n",
       "هدف \"إقرأ-تيك\" هو إعادة إحياء الكتب العربية القديمة من خلال استخدام التقنية الحديثة.  فمن خلال الموقع الإلكتروني، يمكن للجمهور الوصول إلى مخ"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = limit_output_model.generate_content('أكتب مقاله من الف كلمه عن إقرأ-تيك https://eqraatech.com/')\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **هلوسه طبعا** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "يا إقرأ-تيك يا جميل، في دنيانا إنت هتفضل دليل\n",
       "من قلبك العلم بينفجر، والعلم نور ينير ويدجر\n",
       "\n",
       "بإيديك بتنشر المعرفه، وتفتح أبواب عقولنا بصفه\n",
       "من غير حدود ولا قيود، بتوصلنا للعالم بعيد\n",
       "\n",
       "بتعلمنا كل ما نحتاج، بكل سهوله وبدون إرهاق\n",
       "بالتكنولوجيا بتساعدنا، وبتخلينا نتعلم ونتفوق\n",
       "\n",
       "يا إقرأ-تيك يا أملنا، في عالم المعرفه هتنور لنا\n",
       "بتخلينا نفهم ونفكر، ونتقدم معاً ونصبح أقوى بكثير\n",
       "\n",
       "فيك الخير والجمال، فيك النور والسماح\n",
       "يا إقرأ-تيك يا رفيقنا، في رحلة العلم هتكون معنا.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = limit_output_model.generate_content(\" أكتب قصيده باللكنه المصريه ذات قافيه في مدح إقرأ-تيك\")\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore with your own prompts. Try a prompt with a restrictive output limit and then adjust the prompt to work within that limit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Temperature\n",
    "\n",
    "Temperature controls the degree of randomness in token selection. Higher temperatures result in a higher number of candidate tokens from which the next output token is selected, and can produce more diverse results, while lower temperatures have the opposite effect, such that a temperature of 0 results in greedy decoding, selecting the most probable token at each step.\n",
    "\n",
    "Temperature doesn't provide any guarantees of randomness, but it can be used to \"nudge\" the output somewhat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "أحمر \n",
      "\n",
      "أزرق.\n",
      "\n",
      "أزرق \n",
      "\n",
      "أحمر \n",
      "\n",
      "أزرق \n",
      "\n",
      "أحمر \n",
      "\n",
      "أحمر \n",
      "\n",
      "أحمر \n",
      "\n",
      "أزرق. \n",
      "\n",
      "أزرق \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "high_temp_model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash',\n",
    "    generation_config=genai.GenerationConfig(temperature=2.0))\n",
    "\n",
    "for _ in range(10):\n",
    "  response = high_temp_model.generate_content('أختر لون عشوائي... (قم باختيار كلمة واحده فقط)')\n",
    "  if response.parts:\n",
    "    print(response.text)\n",
    "  # Slow down a bit so we don't get Resource Exhausted errors.\n",
    "  time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try the same prompt with temperature set to zero. Note that the output is not completely deterministic, as other parameters affect token selection, but the results will tend to be more stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "أزرق \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "أزرق \n",
      "\n",
      "أزرق \n",
      "\n",
      "أزرق \n",
      "\n",
      "أزرق \n",
      "\n",
      "أزرق \n",
      "\n",
      "أزرق \n",
      "\n",
      "أزرق \n",
      "\n",
      "أزرق \n",
      "\n",
      "أزرق \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "low_temp_model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash',\n",
    "    generation_config=genai.GenerationConfig(temperature=0.0))\n",
    "\n",
    "for _ in range(10):\n",
    "  response = low_temp_model.generate_content('أختر لون عشوائي... (قم باختيار كلمة واحده فقط)')\n",
    "  if response.parts:\n",
    "    print(response.text)\n",
    "\n",
    "  time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **No Creativity** when temp = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Top-K and top-P\n",
    "\n",
    "Like temperature, top-K and top-P parameters are also used to control the diversity of the model's output.\n",
    "\n",
    "Top-K is a positive integer that defines the number of most probable tokens from which to select the output token. A top-K of 1 selects a single token, performing greedy decoding.\n",
    "\n",
    "Top-P defines the probability threshold that, once cumulatively exceeded, tokens stop being selected as candidates. A top-P of 0 is typically equivalent to greedy decoding, and a top-P of 1 typically selects every token in the model's vocabulary.\n",
    "\n",
    "When both are supplied, the Gemini API will filter top-K tokens first, then top-P and then finally sample from the candidate tokens using the supplied temperature.\n",
    "\n",
    "Run this example a number of times, change the settings and observe the change in output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "في قلب القاهرة القديمة، حيث تضج الحارات الضيقة بالأنشطة اليومية، عاش قط أبيض يُدعى \"سنبل\". كان سنبل قطًا قويًا، عضلاته متناسقة، وشعره أبيض ناصع كالثلج، عيونه صفراء حادة تخترق كل شيء. ولكن ما كان يميز سنبل عن غيره هو قلبه الطيب الذي كان يتوق دائمًا لمساعدة الآخرين. \n",
       "\n",
       "كان سنبل يتجول في أنحاء مصر، يزور مدنها وقراها، من القاهرة إلى الإسكندرية، ومن الأقصر إلى أسوان. \n",
       "\n",
       "في رحلته، التقى سنبل بالعديد من الأشخاص والحيوانات، بعضهم كان لطيفًا وودودًا، والبعض الآخر كان عدوانيًا وغير ودي. لكن سنبل لم يتغير، حافظ على طبيعته الكريمة، مستعدًا دائمًا للوقوف بجانب الضعيف، وحماية المظلوم.\n",
       "\n",
       "ذات يوم، وصل سنبل إلى قرية صغيرة في الصعيد. وجد فتاة صغيرة تبكي بشدة، فقد فقدت قطتها المفضلة. \n",
       "\n",
       "تقدم سنبل إليها بهدوء وسألها عن الأمر. شرحت له الفتاة أنها كانت تلعب مع قطتها في الحقل، لكنها انزلقت وسقطت، وتُركت قطتها وحدها. \n",
       "\n",
       "لم يتردد سنبل لحظة واحدة، وبدأ يبحث عن القطة المفقودة. \n",
       "\n",
       "بعد ساعات من البحث، وجد سنبل القطة مختبئة تحت شجرة. \n",
       "\n",
       "أخذ سنبل القطة الصغيرة إلى الفتاة التي اندفعت لاحتضانها.  \n",
       "\n",
       "غمرت الفرحة قلب الفتاة، وعبرت عن شكرها لسنبل بأفضل ما لديها. \n",
       "\n",
       "واصل سنبل رحلته عبر مصر، متجولًا في صحاريها، وشوارعها، وأزقتها، محاكيًا قصصًا عن الكرم والنبل، ونشر الفرح أينما حل. \n",
       "\n",
       "كان سنبل رمزًا للحنان والقوة، دليلًا على أن القلوب الطيبة موجودة في كل مكان، مهما كانت صعوبة الحياة. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash-001',\n",
    "    generation_config=genai.GenerationConfig(\n",
    "        # These are the default values for gemini-1.5-flash-001.\n",
    "        temperature=1.0,\n",
    "        top_k=64,\n",
    "        top_p=0.95,\n",
    "    ))\n",
    "\n",
    "story_prompt = \"قم بكتابه قصه عن قط أبيض ذو شخصيه قويه وقلب طيب يتجول في انحاء مصر\"\n",
    "response = model.generate_content(story_prompt)\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rMrYs1koY6DX"
   },
   "source": [
    "---\n",
    "## Prompting\n",
    "---\n",
    "This section contains some prompts from the chapter for you to try out directly in the API. Try changing the text here to see how each prompt performs with different instructions, more examples, or any other changes you can think of."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhj_tQidZJP7"
   },
   "source": [
    "### 1. Zero-shot\n",
    "\n",
    "Zero-shot prompts are prompts that describe the request for the model directly.\n",
    "\n",
    "<table align=left>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://aistudio.google.com/prompts/1gzKKgDHwkAvexG5Up0LMtl1-6jKMKe4g\"><img src=\"https://ai.google.dev/site-assets/images/marketing/home/icon-ais.png\" style=\"height: 24px\" height=24/> Open in AI Studio</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T13:24:25.262625Z",
     "iopub.status.busy": "2024-11-11T13:24:25.261864Z",
     "iopub.status.idle": "2024-11-11T13:24:25.522715Z",
     "shell.execute_reply": "2024-11-11T13:24:25.521596Z",
     "shell.execute_reply.started": "2024-11-11T13:24:25.262583Z"
    },
    "id": "1_t-cwnDZzbH",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Sentiment: **POSITIVE**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configs = genai.GenerationConfig(\n",
    "        temperature=0.1,\n",
    "        top_p=1,\n",
    "        max_output_tokens=5)\n",
    "\n",
    "model = genai.GenerativeModel('gemini-1.5-flash-001', generation_config=configs)\n",
    "\n",
    "zero_shot_prompt = \"\"\"Classify movie reviews as POSITIVE, NEUTRAL or NEGATIVE.\n",
    "Review: \"Her\" is a disturbing study revealing the direction\n",
    "humanity is headed if AI is allowed to keep evolving,\n",
    "unchecked. I wish there were more movies like this masterpiece.\n",
    "Sentiment: \"\"\"\n",
    "\n",
    "response = model.generate_content(zero_shot_prompt)\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3b5568bdeb11"
   },
   "source": [
    "#### Enum mode\n",
    "\n",
    "The models are trained to generate text, and can sometimes produce more text than you may wish for. In the preceding example, the model will output the label, sometimes it can include a preceding \"Sentiment\" label, and without an output token limit, it may also add explanatory text afterwards.\n",
    "\n",
    "The Gemini API has an [Enum mode](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Enum.ipynb) feature that allows you to constrain the output to a fixed set of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T13:24:32.067597Z",
     "iopub.status.busy": "2024-11-11T13:24:32.066883Z",
     "iopub.status.idle": "2024-11-11T13:24:32.711317Z",
     "shell.execute_reply": "2024-11-11T13:24:32.710065Z",
     "shell.execute_reply.started": "2024-11-11T13:24:32.067553Z"
    },
    "id": "ad118a56c598",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import enum\n",
    "\n",
    "class Sentiment(enum.Enum):\n",
    "    # The response should be one of the following options    \n",
    "    POSITIVE = \"positive\"\n",
    "    NEUTRAL = \"neutral\"\n",
    "    NEGATIVE = \"negative\"\n",
    "\n",
    "configs = genai.GenerationConfig(\n",
    "    response_mime_type=\"text/x.enum\",\n",
    "    response_schema=Sentiment\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "positive"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = genai.GenerativeModel(\n",
    "            'gemini-1.5-flash-001',\n",
    "            generation_config=configs)\n",
    "\n",
    "response = model.generate_content(zero_shot_prompt)\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0udiSwNbv45W"
   },
   "source": [
    "### 2. One-shot and few-shot\n",
    "\n",
    "Providing an example of the expected response is known as a \"one-shot\" prompt. When you provide multiple examples, it is a \"few-shot\" prompt.\n",
    "\n",
    "<table align=left>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://aistudio.google.com/prompts/1jjWkjUSoMXmLvMJ7IzADr_GxHPJVV2bg\"><img src=\"https://ai.google.dev/site-assets/images/marketing/home/icon-ais.png\" style=\"height: 24px\" height=24/> Open in AI Studio</a>\n",
    "  </td>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T13:24:38.971558Z",
     "iopub.status.busy": "2024-11-11T13:24:38.971146Z",
     "iopub.status.idle": "2024-11-11T13:24:39.319086Z",
     "shell.execute_reply": "2024-11-11T13:24:39.317958Z",
     "shell.execute_reply.started": "2024-11-11T13:24:38.971519Z"
    },
    "id": "hd4mVUukwOKZ",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "\"size\": \"large\",\n",
      "\"type\": \"normal\",\n",
      "\"ingredients\": [\"cheese\", \"pineapple\"]\n",
      "}\n",
      "``` \n",
      "\n"
     ]
    }
   ],
   "source": [
    "configs = genai.GenerationConfig(\n",
    "        temperature=0.1,\n",
    "        top_p=1,\n",
    "        max_output_tokens=250,\n",
    "    )\n",
    "\n",
    "model = genai.GenerativeModel('gemini-1.5-flash-latest', generation_config=configs)\n",
    "\n",
    "few_shot_prompt = \"\"\"Parse a customer's pizza order into valid JSON:\n",
    "\n",
    "EXAMPLE:\n",
    "I want a small pizza with cheese, tomato sauce, and pepperoni.\n",
    "JSON Response:\n",
    "```\n",
    "{\n",
    "\"size\": \"small\",\n",
    "\"type\": \"normal\",\n",
    "\"ingredients\": [\"cheese\", \"tomato sauce\", \"peperoni\"]\n",
    "}\n",
    "```\n",
    "\n",
    "EXAMPLE:\n",
    "Can I get a large pizza with tomato sauce, basil and mozzarella\n",
    "JSON Response:\n",
    "```\n",
    "{\n",
    "\"size\": \"large\",\n",
    "\"type\": \"normal\",\n",
    "\"ingredients\": [\"tomato sauce\", \"basil\", \"mozzarella\"]\n",
    "}\n",
    "\n",
    "ORDER:\n",
    "\"\"\"\n",
    "\n",
    "customer_order = \"Give me a large with cheese & pineapple\"\n",
    "\n",
    "response = model.generate_content([few_shot_prompt, customer_order])\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "021293096f08"
   },
   "source": [
    "#### JSON mode\n",
    "\n",
    "To provide control over the schema, and to ensure that you only receive JSON (with no other text or markdown), you can use the Gemini API's [JSON mode](https://github.com/google-gemini/cookbook/blob/main/quickstarts/JSON_mode.ipynb). This forces the model to constrain decoding, such that token selection is guided by the supplied schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T08:14:58.319116Z",
     "iopub.status.busy": "2024-11-11T08:14:58.318683Z",
     "iopub.status.idle": "2024-11-11T08:14:58.741279Z",
     "shell.execute_reply": "2024-11-11T08:14:58.739848Z",
     "shell.execute_reply.started": "2024-11-11T08:14:58.319073Z"
    },
    "id": "50fbf0260912",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import typing_extensions as typing\n",
    "\n",
    "class PizzaOrder(typing.TypedDict):\n",
    "    size: str\n",
    "    ingredients: list[str]\n",
    "    type: str\n",
    "\n",
    "configs = genai.GenerationConfig(\n",
    "        temperature=0.1,\n",
    "        response_mime_type=\"application/json\",\n",
    "        response_schema=PizzaOrder,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"ingredients\": [\"apple\", \"chocolate\"], \"size\": \"large\", \"type\": \"dessert\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = genai.GenerativeModel('gemini-1.5-flash-latest', generation_config=configs)\n",
    "\n",
    "response = model.generate_content(\"Can I have a large dessert pizza with apple and chocolate\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4a93e338e57c"
   },
   "source": [
    "### 3. Chain of Thought (CoT)\n",
    "\n",
    "Direct prompting on LLMs can return answers quickly and (in terms of output token usage) efficiently, but they can be prone to hallucination. The answer may \"look\" correct (in terms of language and syntax) but is incorrect in terms of factuality and reasoning.\n",
    "\n",
    "Chain-of-Thought prompting is a technique where you instruct the model to output intermediate reasoning steps, and it typically gets better results, especially when combined with few-shot examples. It is worth noting that this technique doesn't completely eliminate hallucinations, and that it tends to cost more to run, due to the increased token count.\n",
    "\n",
    "As models like the Gemini family are trained to be \"chatty\" and provide reasoning steps, you can ask the model to be more direct in the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T13:24:47.261345Z",
     "iopub.status.busy": "2024-11-11T13:24:47.260232Z",
     "iopub.status.idle": "2024-11-11T13:24:47.553694Z",
     "shell.execute_reply": "2024-11-11T13:24:47.552589Z",
     "shell.execute_reply.started": "2024-11-11T13:24:47.261287Z"
    },
    "id": "5715555db1c1",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "52 \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now, I\n",
    "am 20 years old. How old is my partner? Return the answer immediately.\"\"\"\n",
    "\n",
    "model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
    "response = model.generate_content(prompt)\n",
    "\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e12b19677bfd"
   },
   "source": [
    "Now try the same approach, but indicate to the model that it should \"think step by step\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T13:24:50.614301Z",
     "iopub.status.busy": "2024-11-11T13:24:50.613897Z",
     "iopub.status.idle": "2024-11-11T13:24:51.230860Z",
     "shell.execute_reply": "2024-11-11T13:24:51.229420Z",
     "shell.execute_reply.started": "2024-11-11T13:24:50.614263Z"
    },
    "id": "ffd7536a481f",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here's how to solve this:\n",
       "\n",
       "* **When you were 4:** Your partner was 3 times your age, meaning they were 4 * 3 = 12 years old.\n",
       "* **Age difference:** The age difference between you and your partner is 12 - 4 = 8 years.\n",
       "* **Your current age:** You are now 20 years old.\n",
       "* **Partner's current age:** Since the age difference remains constant, your partner is 20 + 8 = **28 years old**. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now,\n",
    "I am 20 years old. How old is my partner? Let's think step by step.\"\"\"\n",
    "\n",
    "response = model.generate_content(prompt)\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oiLgBQJj0V53"
   },
   "source": [
    "### 4. ReAct: Reason and act\n",
    "\n",
    "In this example you will run a ReAct prompt directly in the Gemini API and perform the searching steps yourself. As this prompt follows a well-defined structure, there are frameworks available that wrap the prompt into easier-to-use APIs that make tool calls automatically, such as the LangChain example from the chapter.\n",
    "\n",
    "To try this out with the Wikipedia search engine, check out the [Searching Wikipedia with ReAct](https://github.com/google-gemini/cookbook/blob/main/examples/Search_Wikipedia_using_ReAct.ipynb) cookbook example.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T13:24:54.954286Z",
     "iopub.status.busy": "2024-11-11T13:24:54.953228Z",
     "iopub.status.idle": "2024-11-11T13:24:54.961300Z",
     "shell.execute_reply": "2024-11-11T13:24:54.960188Z",
     "shell.execute_reply.started": "2024-11-11T13:24:54.954242Z"
    },
    "id": "cBgyNJ5z0VSs",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_instructions = \"\"\"\n",
    "Solve a question answering task with interleaving Thought, Action, Observation steps. Thought can reason about the current situation,\n",
    "Observation is understanding relevant information from an Action's output and Action can be one of three types:\n",
    " (1) <search>entity</search>, which searches the exact entity on Wikipedia and returns the first paragraph if it exists. If not, it\n",
    "     will return some similar entities to search and you can try to search the information from those topics.\n",
    " (2) <lookup>keyword</lookup>, which returns the next sentence containing keyword in the current context. This only does exact matches,\n",
    "     so keep your searches short.\n",
    " (3) <finish>answer</finish>, which returns the answer and finishes the task.\n",
    "\"\"\"\n",
    "\n",
    "example1 = \"\"\"Question\n",
    "من هو مؤسس مجلة \"الأهرام\" المصرية؟\n",
    "\n",
    "Thought 1\n",
    "السؤال يطلب معرفة من هو مؤسس مجلة \"الأهرام\" المصرية. سأبحث عن \"مؤسس مجلة الأهرام\" على ويكيبيديا.\n",
    "\n",
    "Action 1\n",
    "<search>مؤسس مجلة الأهرام</search>\n",
    "\n",
    "Observation 1\n",
    "مؤسس مجلة الأهرام هو الصحفي المصري محمد علي.\n",
    "\n",
    "Thought 2\n",
    "الفقرة تحتوي على الإجابة. محمد علي هو مؤسس مجلة الأهرام.\n",
    "\n",
    "Action 2\n",
    "<finish>محمد علي</finish>\n",
    "\"\"\"\n",
    "\n",
    "example2 = \"\"\"Question\n",
    "من هو الشاعر المصري الذي كتب قصيدة \"الأطلال\"؟\n",
    "\n",
    "Thought 1\n",
    "السؤال يطلب معرفة من هو الشاعر المصري الذي كتب قصيدة \"الأطلال\". سأبحث عن \"الشاعر الذي كتب قصيدة الأطلال\" في ويكيبيديا.\n",
    "\n",
    "Action 1\n",
    "<search>قصيدة الأطلال</search>\n",
    "\n",
    "Observation 1\n",
    "قصيدة \"الأطلال\" كتبها الشاعر المصري إبراهيم ناجي.\n",
    "\n",
    "Thought 2\n",
    "الفقرة تحتوي على الإجابة. إبراهيم ناجي هو الشاعر الذي كتب قصيدة \"الأطلال\".\n",
    "\n",
    "Action 2\n",
    "<finish>إبراهيم ناجي</finish>\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"Question\n",
    "ما هي أشهر معالم السياحة في مدينة الأقصر؟\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C3wbfstjTgey"
   },
   "source": [
    "To capture a single step at a time, while ignoring any hallucinated Observation steps, you will use `stop_sequences` to end the generation process. The steps are `Thought`, `Action`, `Observation`, in that order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T13:25:01.584042Z",
     "iopub.status.busy": "2024-11-11T13:25:01.583464Z",
     "iopub.status.idle": "2024-11-11T13:25:03.683905Z",
     "shell.execute_reply": "2024-11-11T13:25:03.682672Z",
     "shell.execute_reply.started": "2024-11-11T13:25:01.583948Z"
    },
    "id": "8mxrXRkRTdXm",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought 1\n",
      "السؤال يطلب معرفة أشهر معالم السياحة في مدينة الأقصر. سأبحث عن \"معالم السياحة في الأقصر\" في ويكيبيديا.\n",
      "\n",
      "Action 1\n",
      "<search>معالم السياحة في الأقصر</search>\n",
      "\n",
      "Observation 1\n",
      "تُعرف الأقصر بكونها \"متحفًا مفتوحًا\" بسبب كثرة المعالم التاريخية والآثار الفرعونية فيها. \n",
      "\n",
      "Thought 2\n",
      "الفقرة لا تحتوي على معلومات محددة عن أشهر معالم السياحة، سأبحث عن \"أشهر معالم السياحة في الأقصر\" في ويكيبيديا.\n",
      "\n",
      "Action 2\n",
      "<search>أشهر معالم السياحة في الأقصر</search>\n",
      "\n",
      "Observation 2\n",
      "من أشهر معالم الأقصر: \n",
      "\n",
      "Thought 3\n",
      "الفقرة تحتوي على قائمة ببعض أشهر المعالم السياحية في الأقصر.\n",
      "\n",
      "Action 3\n",
      "<finish>من أشهر معالم الأقصر: معبد الأقصر، معبد الكرنك، وادي الملوك، وادي الملكات، وقرية الغرنقة، ومعبد الأقصر، ومعبد الأقصر.</finish> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
    "react_chat = model.start_chat()\n",
    "\n",
    "# You will perform the Action, so generate up to, but not including, the Observation.\n",
    "config = genai.GenerationConfig()\n",
    "\n",
    "resp = react_chat.send_message(\n",
    "                    [model_instructions, example1, example2, question],\n",
    "                    generation_config=config)\n",
    "\n",
    "print(resp.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bo0tzf4nX6dA"
   },
   "source": [
    "This process repeats until the `<finish>` action is reached. You can continue running this yourself if you like, or try the [Wikipedia example](https://github.com/google-gemini/cookbook/blob/main/examples/Search_Wikipedia_using_ReAct.ipynb) to see a fully automated ReAct system at work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jPiZ_eIIaVPt"
   },
   "source": [
    "---\n",
    "## Code prompting\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZinKamwXeR6C"
   },
   "source": [
    "### 1. Generating code\n",
    "\n",
    "The Gemini family of models can be used to generate code, configuration and scripts. Generating code can be helpful when learning to code, learning a new language or for rapidly generating a first draft.\n",
    "\n",
    "It's important to be aware that since LLMs can't reason, and can repeat training data, it's essential to read and test your code first, and comply with any relevant licenses.\n",
    "\n",
    "<table align=left>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://aistudio.google.com/prompts/1YX71JGtzDjXQkgdes8bP6i3oH5lCRKxv\"><img src=\"https://ai.google.dev/site-assets/images/marketing/home/icon-ais.png\" style=\"height: 24px\" height=24/> Open in AI Studio</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T13:25:12.551884Z",
     "iopub.status.busy": "2024-11-11T13:25:12.550944Z",
     "iopub.status.idle": "2024-11-11T13:25:12.877757Z",
     "shell.execute_reply": "2024-11-11T13:25:12.876668Z",
     "shell.execute_reply.started": "2024-11-11T13:25:12.551839Z"
    },
    "id": "fOQP9pqmeUO1",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "def factorial(n):\n",
       "  if n == 0:\n",
       "    return 1\n",
       "  else:\n",
       "    return n * factorial(n-1)\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash-latest',\n",
    "    generation_config=genai.GenerationConfig(\n",
    "        temperature=1,\n",
    "        top_p=1,\n",
    "        max_output_tokens=1024,\n",
    "    ))\n",
    "\n",
    "# Gemini 1.5 models are very chatty, so it helps to specify they stick to the code.\n",
    "code_prompt = \"\"\"\n",
    "Write a Python function to calculate the factorial of a number. No explanation, provide only the code.\n",
    "\"\"\"\n",
    "\n",
    "response = model.generate_content(code_prompt)\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wlBMWSFhgVRQ"
   },
   "source": [
    "### 2. Code execution\n",
    "\n",
    "The Gemini API can automatically run generated code too, and will return the output.\n",
    "\n",
    "<table align=left>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://aistudio.google.com/prompts/11veFr_VYEwBWcLkhNLr-maCG0G8sS_7Z\"><img src=\"https://ai.google.dev/site-assets/images/marketing/home/icon-ais.png\" style=\"height: 24px\" height=24/> Open in AI Studio</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T13:25:20.197695Z",
     "iopub.status.busy": "2024-11-11T13:25:20.196901Z",
     "iopub.status.idle": "2024-11-11T13:25:22.594758Z",
     "shell.execute_reply": "2024-11-11T13:25:22.593755Z",
     "shell.execute_reply.started": "2024-11-11T13:25:20.197650Z"
    },
    "id": "jT3OfWYfhjRL",
    "trusted": true
   },
   "outputs": [
    {
     "ename": "InternalServerError",
     "evalue": "500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalServerError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m genai\u001b[38;5;241m.\u001b[39mGenerativeModel(\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgemini-1.5-flash-latest\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      3\u001b[0m     tools\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcode_execution\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m code_exec_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124mCalculate the sum of the first 14 prime numbers. Only consider the odd primes, and make sure you get them all.\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m----> 9\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode_exec_prompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m Markdown(response\u001b[38;5;241m.\u001b[39mtext)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp-env/lib/python3.9/site-packages/google/generativeai/generative_models.py:331\u001b[0m, in \u001b[0;36mGenerativeModel.generate_content\u001b[0;34m(self, contents, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation_types\u001b[38;5;241m.\u001b[39mGenerateContentResponse\u001b[38;5;241m.\u001b[39mfrom_iterator(iterator)\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrequest_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    335\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation_types\u001b[38;5;241m.\u001b[39mGenerateContentResponse\u001b[38;5;241m.\u001b[39mfrom_response(response)\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m google\u001b[38;5;241m.\u001b[39mapi_core\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mInvalidArgument \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp-env/lib/python3.9/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py:830\u001b[0m, in \u001b[0;36mGenerativeServiceClient.generate_content\u001b[0;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_universe_domain()\n\u001b[1;32m    829\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[0;32m--> 830\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[1;32m    838\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp-env/lib/python3.9/site-packages/google/api_core/gapic_v1/method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp-env/lib/python3.9/site-packages/google/api_core/retry/retry_unary.py:293\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    290\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[1;32m    292\u001b[0m )\n\u001b[0;32m--> 293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp-env/lib/python3.9/site-packages/google/api_core/retry/retry_unary.py:153\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m     \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43msleep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(sleep)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp-env/lib/python3.9/site-packages/google/api_core/retry/retry_base.py:212\u001b[0m, in \u001b[0;36m_retry_error_helper\u001b[0;34m(exc, deadline, next_sleep, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m predicate_fn(exc):\n\u001b[1;32m    207\u001b[0m     final_exc, source_exc \u001b[38;5;241m=\u001b[39m exc_factory_fn(\n\u001b[1;32m    208\u001b[0m         error_list,\n\u001b[1;32m    209\u001b[0m         RetryFailureReason\u001b[38;5;241m.\u001b[39mNON_RETRYABLE_ERROR,\n\u001b[1;32m    210\u001b[0m         original_timeout,\n\u001b[1;32m    211\u001b[0m     )\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msource_exc\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    214\u001b[0m     on_error_fn(exc)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp-env/lib/python3.9/site-packages/google/api_core/retry/retry_unary.py:144\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sleep \u001b[38;5;129;01min\u001b[39;00m sleep_generator:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 144\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n\u001b[1;32m    146\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(_ASYNC_RETRY_WARNING)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp-env/lib/python3.9/site-packages/google/api_core/timeout.py:120\u001b[0m, in \u001b[0;36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# Avoid setting negative timeout\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout \u001b[38;5;241m-\u001b[39m time_since_first_attempt)\n\u001b[0;32m--> 120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp-env/lib/python3.9/site-packages/google/api_core/grpc_helpers.py:78\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m callable_(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[0;31mInternalServerError\u001b[0m: 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting"
     ]
    }
   ],
   "source": [
    "model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash-latest',\n",
    "    tools='code_execution')\n",
    "\n",
    "code_exec_prompt = \"\"\"\n",
    "Calculate the sum of the first 14 prime numbers. Only consider the odd primes, and make sure you get them all.\n",
    "\"\"\"\n",
    "\n",
    "response = model.generate_content(code_exec_prompt)\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZspT1GSkjG6d"
   },
   "source": [
    "While this looks like a single-part response, you can inspect the response to see the each of the steps: initial text, code generation, execution results, and final text summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T13:25:33.666437Z",
     "iopub.status.busy": "2024-11-11T13:25:33.664999Z",
     "iopub.status.idle": "2024-11-11T13:25:33.672514Z",
     "shell.execute_reply": "2024-11-11T13:25:33.671251Z",
     "shell.execute_reply.started": "2024-11-11T13:25:33.666374Z"
    },
    "id": "j4gQVzcRjRX-",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: \"I will calculate the sum of the first 14 odd prime numbers.\\n\\nFirst, I will define a function to check if a number is prime:\\n\\n\"\n",
      "\n",
      "-----\n",
      "executable_code {\n",
      "  language: PYTHON\n",
      "  code: \"\\ndef is_prime(n):\\n    \\\"\\\"\\\"\\n    Check if a number is prime.\\n    \\\"\\\"\\\"\\n    if n <= 1:\\n        return False\\n    for i in range(2, int(n**0.5) + 1):\\n        if n % i == 0:\\n            return False\\n    return True\\n\"\n",
      "}\n",
      "\n",
      "-----\n",
      "code_execution_result {\n",
      "  outcome: OUTCOME_OK\n",
      "}\n",
      "\n",
      "-----\n",
      "text: \"Now I will loop through the odd numbers, and check if they are prime. I will stop once I find 14 prime numbers.\\n\\n\"\n",
      "\n",
      "-----\n",
      "executable_code {\n",
      "  language: PYTHON\n",
      "  code: \"\\nprimes = []\\nn = 1\\ncount = 0\\nwhile count < 14:\\n    if n % 2 != 0 and is_prime(n):\\n        primes.append(n)\\n        count += 1\\n    n += 2\\n\\nprint(f\\'The first 14 odd primes are: {primes}\\')\\nprint(f\\'The sum of the first 14 odd primes is: {sum(primes)}\\')\\n\"\n",
      "}\n",
      "\n",
      "-----\n",
      "code_execution_result {\n",
      "  outcome: OUTCOME_OK\n",
      "  output: \"The first 14 odd primes are: [3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47]\\nThe sum of the first 14 odd primes is: 326\\n\"\n",
      "}\n",
      "\n",
      "-----\n",
      "text: \"Therefore, the sum of the first 14 odd prime numbers is **326**. \\n\"\n",
      "\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "for part in response.candidates[0].content.parts:\n",
    "  print(part)\n",
    "  print(\"-----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1gUX8QzCj4d5"
   },
   "source": [
    "### 3. Explaining code\n",
    "\n",
    "The Gemini family of models can explain code to you too.\n",
    "\n",
    "<table align=left>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://aistudio.google.com/prompts/1N7LGzWzCYieyOf_7bAG4plrmkpDNmUyb\"><img src=\"https://ai.google.dev/site-assets/images/marketing/home/icon-ais.png\" style=\"height: 24px\" height=24/> Open in AI Studio</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T13:25:38.852525Z",
     "iopub.status.busy": "2024-11-11T13:25:38.852076Z",
     "iopub.status.idle": "2024-11-11T13:25:41.908425Z",
     "shell.execute_reply": "2024-11-11T13:25:41.907267Z",
     "shell.execute_reply.started": "2024-11-11T13:25:38.852484Z"
    },
    "id": "7_jPMMoxkIEb",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "This code snippet defines a Python class named `GitHubDataCollector` that utilizes the GitHub API to gather and analyze data related to GitHub users and repositories, with a focus on Egyptian contributors. \n",
       "\n",
       "Here's a breakdown of what the code does at a high level:\n",
       "\n",
       "**What it is:**\n",
       "\n",
       "* **GitHub Data Scraping:** It's a script designed to extract data from GitHub using its API. \n",
       "* **Focus on Egypt:**  The code specifically targets contributors and repositories from Egypt, making it useful for analyzing the presence of Egyptian developers on the platform. \n",
       "* **Multiple Functions:** It defines functions to:\n",
       "    * Scrape Egyptian users (users from Egypt)\n",
       "    * Scrape repositories owned by Egyptian users\n",
       "    * Scrape top worldwide repositories (excluding those in Egypt)\n",
       "    * Extract Egyptian contributors from top worldwide repositories\n",
       "\n",
       "**Why you would use it:**\n",
       "\n",
       "* **Research:** Analyze the participation of Egyptian developers in the global GitHub community, understand their contributions, and the types of repositories they work on.\n",
       "* **Community Building:**  Identify Egyptian developers and their projects, enabling collaboration and networking.\n",
       "* **Talent Sourcing:**  Recruiting organizations might leverage this script to find skilled developers in Egypt.\n",
       "\n",
       "**Key Points:**\n",
       "\n",
       "* **GitHubAPI Class:**  The code inherits from a `GitHubAPI` class, likely a custom class to interact with the GitHub API. \n",
       "* **Logging and Progress:**  Includes `logging` and `tqdm` for logging activities and tracking progress.\n",
       "* **CSV Handling:** The code uses the `csv` module to read and write scraped data to CSV files.\n",
       "\n",
       "In summary, this Python script is a tool for researchers, community builders, or recruiters who want to analyze and understand the presence of Egyptian developers on GitHub. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_contents = !curl https://raw.githubusercontent.com/code-quests/open-source-analysis-notebook/refs/heads/main/src/data_collection.py\n",
    "\n",
    "explain_prompt = f\"\"\"\n",
    "Please explain what this file does at a very high level. Breifly, What is it, and why would I use it?\n",
    "\n",
    "```\n",
    "{file_contents}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
    "\n",
    "response = model.generate_content(explain_prompt)\n",
    "Markdown(response.text)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "day-1-prompting.ipynb",
   "toc_visible": true
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "nlp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
